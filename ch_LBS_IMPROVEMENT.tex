\chapter{Improvement on methods for the prediction of protein-ligand binding sites}
\label{chap:LBS_IMPROV}

\section*{Preface}

This Chapter explores in detail fifteen non-redundant and scoring variants of the thirteen ligand binding site prediction methods evaluated in \autoref{chap:LBS_COMP}. The negative effect of feeble pocket scoring schemes and redundancy in ligand site prediction is demonstrated through the performance evaluation of these variants relative to their default modes using seven informative metrics. The work in this Chapter was solely carried out by me.

\section*{Publications}

Utgés, J.S. and Barton, G.J. Comparative evaluation of methods for the prediction of protein-ligand binding sites. \textit{J. Cheminform.} \textbf{16}, 126 (2024). \url{https://doi.org/10.1186/s13321-024-00923-z}.

%\section*{Author contributions}

%J.S.U. and G.J.B. conceived, designed, and developed the research. J.S.U. analysed the data. J.S.U. developed the software. J.S.U. and G.J.B. wrote, reviewed and edited the manuscript. G.J.B. secured funding and supervised.

\section{Introduction}

In \autoref{chap:LBS_COMP} the human component of the LIGYSIS dataset was employed to carry out the largest critical assessment of ligand binding site prediction tools to date \cite{UTGES_2024_LBSCOMP}. This evaluation included a set of thirteen methods combining the latest machine learning methods such as VN-EGNN \cite{SESTAK_2024_VNEGNN}, IF-SitePred \cite{CARBERY_2024_IFSP} or GrASP \cite{SMITH_2024_GrASP}, established methods as P2Rank \cite{KRIVAK_2015_P2RANK, KRIVAK_2018_P2RANK}, PRANK \cite{KRIVAK_2015_PRANK} or fpocket \cite{GUILLOUX_2009_FPOCKET, SCHMIDTKE_2010_FPOCKET2} and earlier geometry/energy-based methods such as PocketFinder\textsuperscript{+} \cite{AN_2005_POCKETFINDER}, Ligsite\textsuperscript{+} \cite{HENDLICH_1997_LIGSITE} and Surfnet\textsuperscript{+} \cite{LASKOWSKI_1995_SURFNET}. These methods were thoroughly evaluated at the residue and pocket level using more than ten different metrics.

Beyond ranking the methods by several metrics, \autoref{chap:LBS_COMP} identified VN-EGNN, IF-SitePred and DeepPocket\textsubscript{SEG} as having predicted pockets within very close spatial proximity ($<$5 \AA{}) and high residue overlap (\textit{I\textsubscript{rel}} $>$ 0.5), which hint at redundancy in pocket prediction. Additionally, PUResNet, PocketFinder, Ligsite or Surfnet were highlighted as they do not report scores, nor explicit rank for their predicted pockets. Both of these issues, pocket prediction redundancy and scoring scheme, are likely to have a considerable effect on the methods' performance. This Chapter explores in detail both of these aspects and evaluates the performance of fifteen novel scoring and non-redundant variants of the thirteen methods evaluated in \autoref{chap:LBS_COMP}.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_COMP/PNG/FIG6_PREDICTION_REDUNDANCY_SPLIT_1.png}
    \caption[The issue of redundancy in ligand binding site prediction]{\textbf{The issue of redundancy in ligand binding site prediction.} \textbf{(A)} A set of predictions where 6/10 (60\%) predictions are redundant, resulting in a low recall of 1/5 (20\%) and inflated precision of 7/7 (100\%); \textbf{(B)} When redundancy is removed, only four predictions remain and recall increases to 3/5 (60\%) and precision decreases to 3/4 (75\%).}
    \label{fig:prediction_redundancy}
\end{figure}

Pocket prediction redundancy is defined here as the prediction of pockets with centroids very close in space (\textit{D} $\leq$ 5\AA{}) or with overlapping residues (\textit{I\textsubscript{rel}} $\geq$ 0.75). This indicates multiple predictions of the same potential ligand binding site. Most ligand site prediction tools predict not only the location of the pocket by means of a centroid or pocket residues, but also a pocket confidence, and an associated rank among all the predicted pockets. Ligand site predictors tend to be evaluated by considering the top-\textit{N}, or top-\textit{N}+2 ranking pockets, where \textit{N} is the number of observed sites for a given protein. The redundant prediction of pockets can result in a sub-optimal ranking thus negatively affecting the performance of the predictors.

\autoref{fig:prediction_redundancy} shows an example protein with \textit{N} = 5 observed pockets. A ligand site predictor returns 10 predictions, but the top-7 are all within 3 \AA{} of one of the observed pockets, and $>$12 \AA{} from any of the other four observed pockets. If the top-\textit{N}+2 (top-7) predictions were considered, this would only recall a single unique pocket, as six of the top-7 predictions are redundant. Top-\textit{N}+2 recall would then be 20\% (1/5). Precision, however, within this top-7 would be 100\% (7/7), as the seven predictions are correctly recalling an observed pocket (which happens to be the same). In this case, both the low recall and the high precision are artefacts resulting of the redundancy (\autorefpanel{fig:prediction_redundancy}{ A}). Redundancy in prediction can often result in an overestimate of the precision and an underestimate of the recall. \autorefpanel{fig:prediction_redundancy}{ B} illustrates what happens when redundant predictions are removed, keeping always higher-scoring predictions. When the six redundant predictions (blue stars) are removed, the other three predictions, which are of different pockets, are considered as now fall within the top-\textit{N}+2 predictions. This increases the recall to 60\%, as 3/5 observed pockets are now correctly predicted. However, the precision decreases, as only three out of the four predictions made overlap with an observed pocket. Pocket rank \#2 has a high score but is not observed. This is a \textit{false positive} in this context, however it might be a candidate pocket yet to be resolved and could prove interesting as a drug target.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_COMP/PNG/FIG6_PREDICTION_REDUNDANCY_SPLIT_2.png}
    \caption[Example of redundant predictions]{\textbf{Example of redundant predictions.} Predictions by VN-EGNN, IF-SitePred and PUResNet, on chain D of PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/4z9m}{4Z9M} \cite{PDB_4Z9M} of human creatine kinase (\href{https://www.uniprot.org/uniprotkb/P17540/entry}{P17540}) where \href{https://www.ebi.ac.uk/pdbe-srv/pdbechem/chemicalCompound/show/ADP}{ADP} binds. For this \href{https://www.ebi.ac.uk/pdbe-srv/pdbechem/chemicalCompound/show/ADP}{ADP} binding site, VN-EGNN reports 7 predictions, IF-SitePred 33, and PUResNet a single prediction. These three methods correctly predict this site, however, VN-EGNN and IF-SitePred report redundant pocket predictions, which centroids are very close, $\leq$ 5 \AA{}, in space and residues overlap high ($\geq$ 0.75).}
    \label{fig:prediction_redundancy_examples}
\end{figure}

\autoref{fig:prediction_redundancy_examples} showcases PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/4z9m}{4Z9M} \cite{PDB_4Z9M} of human creatine kinase S-type, mitochondrial (\href{https://www.uniprot.org/uniprotkb/P17540/entry}{P17540}) as an example of this phenomenon, where VN-EGNN and IF-SitePred redundantly predict the same pocket 7 and 33 times, whereas PUResNet returns a single prediction. All three methods correctly predict the site, just the difference is in the number of returned predictions.

\section{Methods}

\subsection{Generation of ``non-redundant'' sets of predictions}

\autorefpanel{fig:pocket_features_2}{ B} shows that prediction redundancy is an issue particularly for VN-EGNN, IF-SitePred, and to a lesser extent DeepPocket\textsubscript{SEG}. To assess the effect that redundancy has on the performance of these methods, non-redundant subsets of predictions were obtained and labelled with the subscript ``NR''. A predicted pocket $i$ is considered redundant if there exists a pocket $j \neq i$ so that the distance between their centroids $D_{i,j} \leq$ 5 \AA{} or their residue overlap $JI_{i,j} >$ 0.75, i.e., they share at least 3/4 (75\%) of their residues. Refer to \autoref{fig:closest_pred_pockets} for the closest predicted sites for each method. Redundancy filtering was carried out for each method keeping always the higher scoring pocket. Redundancy (\%) was calculated as the proportion of redundant pockets relative to the original total number of pockets. VN-EGNN presents the highest percentage of redundant pockets with 9066/13,582 (67\%) redundant pockets, followed by IF-SitePred with 22,232/44,948 (49\%), and DeepPocket\textsubscript{SEG} with 6744/21,718 (31\%). For other methods, redundancy was minimal ($<$1\%).

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG9_CLOSEST_PREDICTED_POCKETS_NEW.png}
    \caption[Closest predicted pockets for each methods]{\textbf{Closest predicted pockets for each method.} LIGYSIS is a reference dataset, not a prediction method. For each method, the two closest predicted pockets across all protein chains are shown. This is the pair of pockets with the minimum Euclidean distance between their centroids. Protein surface is coloured in tan. The larger pocket (more residues) and centroid is coloured in the method colour, and the other in grey. A distance threshold of \textit{D} = 5 \AA{} was selected to determine whether a pocket prediction was redundant. VN-EGNN, IF-SitePred and DeepPocket\textsubscript{SEG} clearly differ from other methods presenting distances $<$ 1 \AA{}. Examples for each method are from top to bottom and left to right: \href{https://www.uniprot.org/uniprotkb/P00492/entry}{P00492} -- PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/3gep}{3GEP} \cite{KEOUGH_2009_HYPOXAN}, chain: B; \href{https://www.uniprot.org/uniprotkb/Q96KS0/entry}{Q96KS0} -- PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/5v1b}{5V1B} \cite{AHMED_2017_HIF}, chain: A; \href{https://www.uniprot.org/uniprotkb/P31645/entry}{P31645} -- PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/5i73}{5I73} \cite{COLEMAN_2016_SEROTONIN}, chain: A; \href{https://www.uniprot.org/uniprotkb/Q04724/entry}{Q04724} -- PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/1gxr}{1GXR} \cite{PICKLES_2002_WD40}, chain: A; \href{https://www.uniprot.org/uniprotkb/Q5W0Z9/entry}{Q5W0Z9} -- PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/7khm}{7KHM} \cite{CHULJIN_2022_ACYLCOA}, chain: B; \href{https://www.uniprot.org/uniprotkb/Q06187/entry}{Q06187} -- PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/1b55}{1B55} \cite{BARALDI_1999_PH}, chain: B; \href{https://www.uniprot.org/uniprotkb/Q9UQG0/entry}{Q9UQG0} -- PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/7sr6}{7SR6} \cite{BALDWIN_2022_HERVK}, chain: G; \href{https://www.uniprot.org/uniprotkb/P13866/entry}{P13866} -- PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/7yni}{7YNI} \cite{CUI_2023_SGLT}, chain: A; \href{https://www.uniprot.org/uniprotkb/Q14534/entry}{Q14534} -- PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/6c6p}{6C6P} \cite{PADYANA_2019_EPOXIDASE}, chain: A; \href{https://www.uniprot.org/uniprotkb/P31321/entry}{P31321} -- PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/4din}{4DIN} \cite{ILOUZ_2012_PKA}, chain: B; \href{https://www.uniprot.org/uniprotkb/P78527/entry}{P78527} -- PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/7sud}{7SUD} \cite{LIU_2022_DNAPK}, chain: A; \href{https://www.uniprot.org/uniprotkb/Q14416/entry}{Q14416} -- PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/7epb}{7EPB} \cite{DU_2021_MGLU7}, chain: A.}
    \label{fig:closest_pred_pockets}
\end{figure}

\subsection{Pocket re-scoring strategies}

PUResNet, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+} do not score, nor explicitly rank their pockets, and so they were taken in the order given by their pocket ID. This means that when sorting across the dataset, the order of all pockets with the same rank is arbitrary. To obtain a score for these pockets, multiple strategies were employed. Firstly, a score was obtained as the number of pocket amino acids, resulting in variants PUResNet\textsubscript{AA}, PocketFinder\textsuperscript{+}\textsubscript{AA}, Ligsite\textsuperscript{+}\textsubscript{AA} and Surfnet\textsuperscript{+}\textsubscript{AA}. Secondly, PRANK pocket scoring was employed, resulting in variants PUResNet\textsubscript{PRANK}, PocketFinder\textsuperscript{+}\textsubscript{PRANK}, Ligsite\textsuperscript{+}\textsubscript{PRANK} and Surfnet\textsuperscript{+}\textsubscript{PRANK}. IF-SitePred uses a simple pocket scoring scheme, whtih assigns to each centroid the number of clustered cloud points it results from. In this Chapter, newly defined IF-SitePred pocket scores were calculated were calculated as the sum of squares (SS) of the ligandability scores ($LS_{i}$), calculated with \autoref{eq:IFSP_score}, of the $K$ residues on a site (\autoref{eq:IFSP_pocket_score}) resulting in IF-SitePred\textsubscript{RESC}. For PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+}, and Surfnet\textsuperscript{+} the same was done but instead of residue scores, grid point scores ($GS_{i}$) were used (\autoref{eq:leagcy_methos_pocket_score}). This resulted in further variants PocketFinder\textsuperscript{+}\textsubscript{SS}, Ligsite\textsuperscript{+}\textsubscript{SS}, and Surfnet\textsuperscript{+}\textsubscript{SS}. This is the same approach introduced by Krivák \textit{et al.} \cite{KRIVAK_2015_P2RANK} and later adopted by Smith \textit{et al.} \cite{SMITH_2024_GrASP}.

\begin{equation}
SS_{\text{IF--SitePred}} = \sum_{i=1}^{K} LS_i^2
\label{eq:IFSP_pocket_score}
\end{equation}

\begin{equation}
SS_{\text{PocketFinder}^+} = SS_{\text{Ligsite}^+} = SS_{\text{Surfnet}^+} = \sum_{i=1}^{K} GS_i^2
\label{eq:leagcy_methos_pocket_score}
\end{equation}

\subsection{Performance evaluation}

This Chapter evaluates the performance of fifteen novel non-redundant and scoring variants of the thirteen canonical ligand binding site prediction methods surveyed in \autoref{chap:LBS_COMP}. These variants do not affect prediction at the residue level, so performance is only assessed at the pocket level. Mainly three metrics are discussed in this Chapter. Recall (\autoref{eq:recall}) is the percentage of observed binding sites in the reference data that are correctly predicted by a given method for a given DCC, Rank or \textit{I\textsubscript{rel}} threshold. Precision (\autoref{eq:precision}) is the percentage of predicted sites that are correct, i.e., match a pocket in the reference. In this case, Precision\textsubscript{1K} is reported, where all predictions across the LIGYSIS reference set are sorted by score and precision reported as predictions are considered up to the 1000\textsuperscript{th} highest scoring prediction. In a similar way, ROC100 \cite{WEBBER_2003_ROC100} reports cumulative TP \textit{vs} cumulative FP until 100 FP are reached. Finally, relative residue overlap, RRO, (\autoref{eq:RRO}) and relative volume overlap, RVO, (\autoref{eq:RVO}) represent how well the predicted sites match the observed site in terms of residue overlap and shape (\%). Refer to \autoref{subsub:pocket_level_metrics} for more details.

\FloatBarrier

\begin{figure}[htb!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG3_SCORE_vs_RANK_SPLIT1.png}
    \caption[Pocket score \textit{vs} pocket ranking]{\textbf{Pocket score \textit{vs} pocket ranking.} \textbf{(A)} VN-EGNN reported pocket scores; \textbf{(B)} Non-redundant VN-EGNN predictions (VN-EGNN\textsubscript{NR}); \textbf{(C)} Default IF-SitePred predictions are ranked based on the number of pocket cloud points; \textbf{(D)} Non-redundant variant of IF-SitePred (IF-SitePred\textsubscript{NR}); \textbf{(E)} Re-scored non-redundant IF-SitePred predictions (IF-SitePred\textsubscript{RESC-NR}). Score is calculated as sum of squares of residue ligandability scores (\autoref{eq:IFSP_pocket_score}); \textbf{(F)} GrASP; \textbf{(G)} PUResNet does not score its pockets. PUResNet\textsubscript{AA}. This variant uses the number of pocket amino acids as a score; \textbf{(H)} PRANK scored PUResNet pockets; \textbf{(I)} DeepPocket\textsubscript{SEG}; \textbf{(J)} Non-redundant DeepPocket\textsubscript{SEG} predictions (DeepPocket\textsubscript{SEG-NR}); \textbf{(K)} DeepPocket\textsubscript{RESC}; \textbf{(L)} P2Rank\textsubscript{CONS}. (\textbf{d}) and (\textit{v}) indicate whether methods are default, or a variant generated in this work.}
    \label{fig:pocket_score_vs_rank1}
\end{figure}

\section{Results}

\subsection{Effect of redundancy and pocket score on ranking}

Because of \textit{K} = 8 virtual nodes are used in the default VN-EGNN implementation, a maximum of \textit{N} = 8 predicted pockets are possible. However, only seven are observed in our dataset, i.e., in all cases at least one virtual node gets clustered with another, resulting in 7 ``unique'' predictions. \autorefpanel{fig:pocket_score_vs_rank1}{ A} illustrates the issue of redundancy in pocket predictions and how it affects the scoring and ranking of the pockets. A prediction of the same pocket is reported multiple times as distinct virtual nodes, or pocket centroids, which are very close to each other, and present very similar scores. This is why there is no apparent difference in the distribution of scores across the pocket ranks for VN-EGNN, unlike all other methods. After removing redundancy and obtaining VN-EGNN\textsubscript{NR}, this is no longer the case (\autorefpanel{fig:pocket_score_vs_rank1}{ B}).

\begin{figure}[htb!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG3_SCORE_vs_RANK_SPLIT2.png}
    \caption[Pocket score \textit{vs} pocket ranking]{\textbf{Pocket score \textit{vs} pocket ranking.} \textbf{(A)} P2Rank; \textbf{(B)} fpocket\textsubscript{PRANK}; \textbf{(C)} fpocket. This distribution differs massively from the re-scored fpocket\textsubscript{PRANK} one; \textbf{(D)} PocketFinder\textsuperscript{+} does not report pocket scores, so the number of pocket residues is displayed for the PocketFinder\textsuperscript{+}\textsubscript{AA} variant; \textbf{(E)} PocketFinder\textsuperscript{+}\textsubscript{PRANK}; \textbf{(F)} PocketFinder\textsuperscript{+}\textsubscript{SS}. This variant uses the pocket grid points’ scores to calculate a pocket score by summing the squared scores (\autoref{eq:leagcy_methos_pocket_score}); \textbf{(G)} Just like PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} does not score pockets, Y-axis is number of pocket residues (Ligsite\textsuperscript{+}\textsubscript{AA}); \textbf{(H)} Ligsite\textsuperscript{+}\textsubscript{PRANK}; \textbf{(I)} Ligsite\textsuperscript{+}\textsubscript{SS}; \textbf{(J)} Surfnet\textsubscript{+}\textsubscript{AA}; \textbf{(K)} Surfnet\textsuperscript{+}\textsubscript{PRANK}; \textbf{(L)} Surfnet\textsuperscript{+}\textsubscript{SS}. (\textbf{d}) and (\textit{v}) indicate whether methods are default, or a variant generated in this work.}
    \label{fig:pocket_score_vs_rank2}
\end{figure}

IF-SitePred predictions are also highly redundant. However, these pockets, despite being close to each other, present different scores (number of points). That is why higher ranks (1, 2, 3...) present higher scores (\autorefpanel{fig:pocket_score_vs_rank1}{ C}). Redundancy removal can be observed in \autorefpanel{fig:pocket_score_vs_rank1}{ D} as the scatter plot is less crowded and the maximum rank across the dataset is 60, as opposed to 120. \autorefpanel{fig:pocket_score_vs_rank1}{ E} shows the non-redundant set of re-scored IF-SitePred predictions, IF-SitePred\textsubscript{RESC-NR}. This distribution is wider, i.e., scores take values from a larger distribution of values, which might yield a better scoring scheme.

There is no clear difference between \autorefpanel{fig:pocket_score_vs_rank1}{ G-H}, meaning that using PRANK to score PUResNet predictions does not alter the overall ranking of the predictions made within a protein. This makes sense, as only 10\% of proteins present $>$1 predicted pocket. This new score, however, could help in the ranking of pockets across the dataset, and not just within a protein.

The distribution of scores does not change when removing the redundancy from Deep-Pocket\textsubscript{SEG} predictions (\autorefpanel{fig:pocket_score_vs_rank1}{ I-J}), but the maximum rank goes from 200 to 140 indicating the decrease in total predictions. The score distributions of fpocket\textsubscript{PRANK} (\autorefpanel{fig:pocket_score_vs_rank2}{ B}) and fpocket (\autorefpanel{fig:pocket_score_vs_rank2}{ C}) are completely different which means the ranking of pockets, and therefore recall and precision might differ considerably between these two scoring schemes of the same predictions.

The score distributions of ``\textsubscript{AA}'', ``\textsubscript{SS}'' and ``\textsubscript{PRANK}'' variants of PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+} are similar, suggesting that the number of pocket amino acids might dictate the order in which these pockets are reported \autorefpanel{fig:pocket_score_vs_rank2}{ D-L} and that re-scoring the predictions by these methods might not have an effect on their performance.

\subsection{Effect of redundancy and pocket score on recall}

\autoref{fig:pocket_score_vs_rank1} and \autoref{fig:pocket_score_vs_rank2} demonstrate how removing redundancy from predictions can have a drastic effect in the ranking of the predictions, with VN-EGNN being the clearest example. The following analysis explores the effect that redundancy removal and different pocket scoring schemes might have on recall for PUResNet, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}, which do not report pocket scores.

\autorefpanel{fig:pocker_recall_variants1}{ A-B} shows a significant +5.2\% increase in recall after removing redundancy for VN-EGNN predictions (Recall = 46.1\%). This increase corresponds to 346 extra predictions that fall within the top-\textit{N}+2 after removing redundancy. An even stronger improvement can be observed for IF-SitePred (\autorefpanel{fig:pocker_recall_variants1}{ C-D}), where a combination of redundancy removal and pocket re-scoring (\autoref{eq:IFSP_pocket_score}) results in a significant increase of +13.4\% (Recall = 39.1\%), corresponding to 901 extra predictions within the top-\textit{N}+2.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG5_RECALL_VARIANTS_1.png}
    \caption[Recall curves for method variants (I)]{\textbf{Recall curves for method variants (I).} Recall curves for different scoring and ranking variants for VN-EGNN \textbf{(A-C)}, IF-SitePred \textbf{(D-F)}, PUResNet \textbf{(G-I)} and DeepPocket\textsubscript{SEG} \textbf{(J-L)}. For each method, panels illustrate how recall changes as DCC, rank and \textit{I\textsubscript{rel}} thresholds vary. In this last one \textit{I\textsubscript{rel}} is the criterion used to classify predictions. Dashed lines indicate the thresholds used as reference in this work: DCC = 12 \AA{}, rank = top-\textit{N}+2, and \textit{I\textsubscript{rel}} = 0.5. (\textbf{d}) and (\textit{v}) indicate whether methods are default, or variants.}
    \label{fig:pocker_recall_variants1}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG5_RECALL_VARIANTS_2.png}
    \caption[Recall curves for method variants (II)]{\textbf{Recall curves for method variants (II).} Recall curves for different scoring and ranking variants for PocketFinder\textsuperscript{+} \textbf{(A-C)}, Ligsite\textsuperscript{+} \textbf{(D-F)} and Surfnet\textsuperscript{+} \textbf{(G-I)}. For each method, panels illustrate how recall changes as DCC, rank and \textit{I\textsubscript{rel}} thresholds vary. In this last one \textit{I\textsubscript{rel}} is the criterion used to classify predictions. Dashed lines indicate the thresholds used as reference in this work: DCC = 12 \AA{}, rank = top-\textit{N}+2, and \textit{I\textsubscript{rel}} = 0.5. (\textbf{d}) and (\textit{v}) indicate whether methods are default, or variants.}
    \label{fig:pocker_recall_variants2}
\end{figure}

\noindent
Most of this change is due to the redundancy removal, as can be seen by the higher recall of IF-SitePred\textsubscript{NR}. Scoring of PUResNet predictions using the number of pocket amino acids (PUResNet\textsubscript{AA}) or PRANK (PUResNet\textsubscript{PRANK}) had no effect on the recall. This was expected as PUResNet predicts a single pocket in 90\% of the cases, and therefore, there is no strong need for a score to sort predictions within a protein. (\autorefpanel{fig:pocker_recall_variants1}{ G-H}). Just like VN-EGNN and IF-SitePred, the recall of DeepPocket\textsubscript{SEG} benefits from redundancy removal, increasing by +5.6\% (\autorefpanel{fig:pocker_recall_variants1}{ J-K}). For PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}, none of the variants had a significant improvement in the recall (\autorefpanel{fig:pocker_recall_variants2}{ A-I}). This is expected as these predict only a few non-redundant sites per protein, with medians ranging 1-3 pockets per protein. These pockets, might already be sorted by number of amino acids as suggested by \autorefpanel{fig:pocket_score_vs_rank2}{ D-L}.

\subsection{Effect of redundancy and pocket score on \# TP\textsubscript{100 FP}}

There are no negative predictions, either true (TN) or false (FN) in the context of ligand binding site prediction at the pocket level and accordingly, standard ROC/AUC curves cannot be obtained. Only positives are predicted (pockets). FN can be obtained by examining the observed pockets that are not predicted, but there are not scores for them. ROC100 curves provide an alternative to observe the relationship between true (TP) and false positives (FP). Predictions for each method across the whole reference dataset, LIGYSIS, were sorted based on pocket score and cumulative TP and FPs were counted until a certain number of FP was reached, in this case, 100. This visualisation provides insight into how well high-scoring predictions match the ground truth. A higher number of TP at FP = 100 indicates that the high scoring pockets recapitulate well the ground truth, whereas a low number indicates that the high scoring pockets do not match with the observed data, given the used threshold of DCC $\leq$ 12 \AA{}. It is important to understand that FPs in this context might not represent wrong predictions, but could be binding sites that are not considered in our ground truth dataset, that is comprised by biologically relevant protein-ligand interactions as defined in BioLiP, or relevant sites that simply have not been experimentally determined yet. It is also important to contextualise this metric with success rate, or recall, i.e., how many of the observed sites are predicted by each method given the above-mentioned threshold, as well as a rank threshold: top-\textit{N}+2. A method might present a high number of TP within the first 100 FP yet have a low recall overall. \autoref{fig:pocket_ROC100_variants} explores how ROC100 changes for the non-redundant ``\textsubscript{NR}'' and re-scored ``\textsubscript{AA}'', ``\textsubscript{PRANK}'', ``\textsubscript{SS}'', and ``\textsubscript{RESC}'' sets of VN-EGNN, IF-SitePred, PUResNet, DeepPocket\textsubscript{SEG}, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG6_TP100FP_VARIANTS.png}
    \caption[ROC100 curves for non-redundant and re-scored variants]{\textbf{ROC100 curves for non-redundant and re-scored variants.} For each method, predicted pockets across the whole dataset, i.e., all LIGYSIS protein chains, are ranked by their score. This way, pockets with the highest scores will be at the top of the list, whereas pockets with the lowest scores will be at the bottom. This ranking will not correspond to ranking pockets across the dataset by their rank, as a pocket ranked \#2, \#3 or lower could have a higher score than a pocket \#1 of a different protein. Each method has a colour assigned, and each variant resulting of redundancy removal or pocket (re-)scoring a different line style. \textbf{(A)} VN-EGNN and ``\textsubscript{NR}'' variant; \textbf{(B)} IF-SitePred and ``\textsubscript{NR}'', re-scored ``\textsubscript{RESC}'' and ``\textsubscript{RESC-NR}'' variants; \textbf{(C)} PUResNet and ``\textsubscript{AA}'' and ``\textsubscript{PRANK}'' variants; \textbf{(D)} DeepPocket\textsubscript{SEG} and ``\textsubscript{NR}'' variant; \textbf{(E)} PocketFinder\textsuperscript{+} and ``\textsubscript{AA}'', ``\textsubscript{PRANK}'' and ``\textsubscript{SS}'' variants; \textbf{(F)} Ligsite\textsuperscript{+} and variants; \textbf{(G)} Surfnet\textsuperscript{+} and variants.}
    \label{fig:pocket_ROC100_variants}
\end{figure}

%\FloatBarrier

\autorefpanel{fig:pocket_ROC100_variants}{ A} illustrates how redundancy can be misleading and overestimate the performance of VN-EGNN. Removing redundancy results in Δ\textsubscript{TP} = \textminus273 (TP = 1028). This is because redundant predictions by VN-EGNN are very close in space and present very similar scores \autorefpanel{fig:pocket_score_vs_rank1}{ A}. Because of this, in the redundant default set of predictions, multiple TP counts are being added for predictions of the same observed pocket. Even with redundancy removed, VN-EGNN reached 1028 TP for the first 100 FP, indicating that the non-redundant higher scoring pockets recapitulate well the observed data.

There is no difference between IF-SitePred and IF-SitePred\textsubscript{NR} (curves overlap completely), which indicates that despite the redundancy in predictions by this method, its scoring scheme can sort sites in a meaningful manner. Considering multiple proteins with redundant predictions for IF-SitePred: the scoring scheme allows for the top-1 site of each of these proteins to rank above any of the other redundant predictions of the other proteins. The re-scored and non-redundant set of IF-SitePred predictions, IF-SitePred\textsubscript{RESC-NR}, results in a Δ\textsubscript{TP} = +285 (TP = 1246), indicating that IF-SitePred could benefit from a more sophisticated scoring scheme, rather than the number of cloud points per binding site (\autorefpanel{fig:pocket_ROC100_variants}{ B}).

\autorefpanel{fig:pocket_ROC100_variants}{ C} is a perfect example of the importance of scoring pocket predictions. PUResNet does not score its predictions. For this reason, within a protein, pockets have been ranked based on the order they are reported, i.e., on their identifier. When sorting across the whole dataset, pockets with the same ID or rank where randomly shuffled. A massive increase in TP could be observed when simply sorting by the number of pocket residues and using PRANK to score this pockets provides an even larger increment in TP (Δ\textsubscript{TP} = +563) (TP = 1097). An application of this could be running PUResNet on a list of potential drug target proteins. It would add great value to be able to rank the predictions among the targets to decide on a target.

The curve does not change much for DeepPocket\textsubscript{SEG}, (Δ\textsubscript{TP} = \textminus27) (TP = 643), indicating that despite the segmentation module of DeepPocket might result in overlapping pockets, their scoring scheme is robust. It is important to consider that the pocket score results from re-scoring the fpocket candidates, which are not redundant. The redundancy in DeepPocket\textsubscript{SEG} is therefore unrelated to their scoring scheme. These results suggest that there is a big difference between fpocket candidates and extracted DeepPocket pockets and it might not be appropriate to consider the score of the former for the latter (\autorefpanel{fig:pocket_ROC100_variants}{ D}).

For the last three methods, earlier and geometry/energy-based, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}, the results agree in that simply using the number of pocket amino acids results in the maximum TP for 100 FP: Δ\textsubscript{TP} = +114 (TP = 178) (\autorefpanel{fig:pocket_ROC100_variants}{ F}), Δ\textsubscript{TP} = +44 (TP = 159) (\autorefpanel{fig:pocket_ROC100_variants}{ G}) and Δ\textsubscript{TP} = +247 (TP = 308) (\autorefpanel{fig:pocket_ROC100_variants}{ H}). This is surprising, as sum of squares ``\textsubscript{SS}'' and ``\textsubscript{PRANK}'' scoring schemes have worked better for other methods. This result might be related to the fact that pockets predicted by these three methods tend to be larger than those predicted by other methods.

\subsection{Effect of redundancy and pocket score on precision}

For the same reason as why ROC/AUC curves cannot be calculated for ligand binding site prediction (at the pocket level), precision-recall (PR)/AUC curves cannot be either, as false negatives (FN) are not predicted, and therefore not scored. Nevertheless, precision, as the ratio of TP/TP+FP, can be measured. For this, as it was done for ROC100, all predictions for a method were sorted by pocket score and precision calculated as more predictions are considered.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG7_PRECISION_VARIANTS.png}
    \caption[Precision\textsubscript{1K} curves for non-redundant and re-scored variants]{\textbf{Precision\textsubscript{1K} curves for non-redundant and re-scored variants.}  Precision\textsubscript{1K} represents the precision (\%) calculated for the top-scoring 1000 predictions. For each method, predicted pockets across the whole LIGYSIS set are ranked by their score. This way, pockets with the highest scores are at the top of the list, whereas pockets with the lowest scores at the bottom. Each method has a colour assigned, and each scoring variant its own line style. Δ\textsubscript{Precision} indicates the difference in precision between the selected method variant and the default one.  \textbf{(A)} VN-EGNN and ``\textsubscript{NR}'' variant; \textbf{(B)} IF-SitePred and ``\textsubscript{NR}'', re-scored ``\textsubscript{RESC}'' and ``\textsubscript{RESC-NR}'' variants; \textbf{(C)} PUResNet and ``\textsubscript{AA}'' and ``\textsubscript{PRANK}'' variants; \textbf{(D)} DeepPocket\textsubscript{SEG} and ``\textsubscript{NR}'' variant; \textbf{(E)} PocketFinder\textsuperscript{+} and ``\textsubscript{AA}'', ``\textsubscript{PRANK}'' and ``\textsubscript{SS}'' variants; \textbf{(F)} Ligsite\textsuperscript{+} and variants; \textbf{(G)} Surfnet\textsuperscript{+} and variants. Error bars indicate 95\% CI of the precision (proportion) and are displayed every 100 predictions.}
    \label{fig:pocket_precision_variants}
\end{figure}

%\FloatBarrier

\autoref{fig:pocket_precision_variants} portrays the precision curve for the top-1000 predictions for the non-redund-ant and re-scored variants for VN-EGNN, IF-SitePred, PUResNet, DeepPocket\textsubscript{SEG}, Pocket-Finder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}. There is no significant (\textit{p} $>$ 0.05) change in precision between VN-EGNN and VN-EGNN\textsubscript{NR} within the first 1000 predictions, precision = 91.5\% (\autorefpanel{fig:pocket_precision_variants}{ A}). The same can be said for IF-SitePred with a precision = 94.3\% (\autorefpanel{fig:pocket_precision_variants}{ B}). Using PRANK to score PUResNet pockets results in a significant +11.7\% increase in precision of the top-1000 predictions (precision = 93.3\%) (\autorefpanel{fig:pocket_precision_variants}{ C}). DeepPocket\textsubscript{SEG-NR}, as the other redundant methods, does not experience a significant change in precision as redundancy is removed (precision = 81.6\%) (\autorefpanel{fig:pocket_precision_variants}{ D}). For PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}, using the number of pocket amino acids results, ``\textsubscript{AA}'', in a precision increase of +23.3\% (precision = 65.3\%), (\autorefpanel{fig:pocket_precision_variants}{ E}), +16.5\% (precision = 68.8\%) (\autorefpanel{fig:pocket_precision_variants}{ F}) and 29.1\% (precision = 68.8\%) (\autorefpanel{fig:pocket_precision_variants}{ G}), respectively. 

\subsection{Evaluation of predictive performance}

\autoref{fig:pocket_level_benchmark_variants} and \autoref{tab:pocket_level_benchmark_variants} compare the performance of the thirteen methods evaluated in this Chapter, which now include six canonical methods (\textbf{d}) and seven novel variants first introduced in this Chapter (\textit{v}) \autoref{tab:methods_improvement_summary}. In terms of recall, the ranking of the methods does not change much, as the increase obtained by the re-scoring and non-redundant variants, though considerable (+13.4\% for IF-SitePred) is not enough to reach the recall achieved by the top-performing methods (\autorefpanel{fig:pocket_level_benchmark_variants}{ A-C}). However, a shift in the ranking can be observed when Precision\textsubscript{1K} and \# TP\textsubscript{100 FP}. PUResNet, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}, which originally did not score their predictions, benefit greatly of scoring their predictions by simply using the number of amino acids of predicted pockets as a score. This results in increases of up to $\approx$560 TP at 100 FP for PUResNet\textsubscript{PRANK} and 29.1\% in precision for Surfnet\textsuperscript{+}\textsubscript{AA}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG8_POCKET_LEVEL_BENCHMARK_IMPROVED.png}
    \caption[Ligand binding site prediction at the pocket level (variants)]{\textbf{Ligand binding site prediction at the pocket level (variants).} Only the top-performing, i.e., highest top-\textit{N}+2 recall, variant of each method is drawn on this figure, e.g., IF-SitePred\textsubscript{RESC-NR} or VN-EGNN\textsubscript{NR} instead of their default modes. \textbf{(A)} Top-\textit{N}+2 recall, percentage of observed sites that are correctly predicted by a method according to a DCC threshold. Reported recall on \autoref{tab:pocket_level_benchmark_variants} corresponds to DCC = 12 \AA{}; \textbf{(B)} Recall using DCC = 12 \AA{} but considering increasing rank thresholds. ``\textit{all}'' represents the maximum recall of a method, obtained by considering all predictions, regardless of their rank or score; \textbf{(C)} Recall curve using \textit{I\textsubscript{rel}} as a criterion; \textbf{(D)} ROC100 curve (cumulative TP against cumulative FP until 100 FP are reached); \textbf{(E)} Precision curve for the top-1000 predictions of each method across the LIGYSIS dataset. Error bars represent 95\% CI of the recall \textbf{(A-C)} and precision \textbf{(E)}, which are 100 $\times$ proportion. Numbers at the right of the panels indicate groups or blocks of methods that perform similarly for each metric. Stars ``*'' indicate outlier methods, or methods that perform very differently than the rest.}
    \label{fig:pocket_level_benchmark_variants}
\end{figure}

%\vspace{-12pt} % Adjust this value as needed
%\vspace{-12pt} % Adjust this value as needed

\begingroup
\captionsetup{belowskip=0pt,aboveskip=9pt} % or some smaller space ,aboveskip=4pt
\begin{landscape}
\begin{longtable}[c]{|M{33mm}|M{28mm}|M{31mm}|M{26mm}|M{29mm}|M{19mm}|M{18mm}|M{18mm}|}
\hline
\textbf{Method}         & \textbf{Recall\textsubscript{top-\textit{N}}} (\%) & \textbf{Recall\textsubscript{top-\textit{N}+2}} (\%) & \textbf{Recall\textsubscript{max}} (\%) & \textbf{Precision\textsubscript{1K}} (\%) & \textbf{\# TP\textsubscript{100 FP}} & \textbf{RRO} (\%) & \textbf{RVO} (\%) \\ \hline
\endfirsthead
%
\footnotesize{(\textit{v})} VN-EGNN\textsubscript{NR}          & 44.5 (\#7)           & 46.1 (\#11)             & 46.3 (\#11)         & 91.5 (\#4)           & 1028 (\#3)       & \textbf{\textcolor{CBBlue}{31.6 (\#11)}}     & \textbf{\textcolor{CBBlue}{26.7 (\#11)}}     \\ \hline
\footnotesize{(\textit{v})} IF-SitePred\textsubscript{RESC-NR} & \textbf{\textcolor{CBOrange}{29.7 (\#12)}}           & \textbf{\textcolor{CBOrange}{39.1 (\#13)}}             & 51.6 (\#12)           & \textbf{\textcolor{CBBlue}{94.3 (\#1)}}           & \textbf{\textcolor{CBBlue}{1246 (\#1)}}       & 49.3 (\#10)     & 43.7 (\#9)     \\ \hline
\footnotesize{(\textbf{d})} GrASP              & 48.0 (\#2)             & 49.9 (\#5)             & 50.0 (\#8)           & 92.5 (\#3)           & 1017 (\#4)       & 54.5 (\#7)     & 59.8 (\#6)     \\ \hline
\footnotesize{(\textit{v})} PUResNet\textsubscript{PRANK}      & 40.8 (\#10)           & 41.1 (\#12)             & \textbf{\textcolor{CBOrange}{41.1 (\#12)}}         & 93.3 (\#2)           & 1092 (\#2)       & 61.0 (\#4)     & 63.9 (\#4)     \\ \hline
\footnotesize{(\textit{v})} DeepPocket\textsubscript{SEG-NR}   & 43.4 (\#8)          & 49.4 (\#6)            & 55.4 (\#5)        & 81.6 (\#7)          & 643 (\#6)        & 58.4 (\#5)     & 61.3 (\#5)     \\ \hline
\footnotesize{(\textbf{d})} DeepPocket\textsubscript{RESC}     & 46.6 (\#4)           & 58.1 (\#2)            & 89.3 (\#2)        & 81.7 (\#6)          & 637 (\#7)        & 52.6 (\#9)     & 38.2 (\#10)     \\ \hline
\footnotesize{(\textbf{d})} P2Rank\textsubscript{CONS}         & \textbf{\textcolor{CBBlue}{48.8 (\#1)}}           & 53.9 (\#3)             & 57.0 (\#3)           & 90.7 (\#5)           & 932 (\#5)         & 56.4 (\#6)     & 43.8 (\#8)     \\ \hline
\footnotesize{(\textbf{d})} P2Rank             & 46.7 (\#3)           & 51.9 (\#4)             & 57.0 (\#4)           & 79.2 (\#8)           & 586 (\#8)         & 54.4 (\#8)     & 58.2 (\#7)   \\ \hline
\footnotesize{(\textbf{d})} fpocket\textsubscript{PRANK}       & \textbf{\textcolor{CBBlue}{48.8 (\#1)}}           & \textbf{\textcolor{CBBlue}{60.4 (\#1)}} & \textbf{\textcolor{CBBlue}{91.3 (\#1)}}         & 81.7 (\#6)           & 526 (\#9)        & 52.6 (\#9)     & 38.2 (\#10)     \\ \hline
\footnotesize{(\textbf{d})} fpocket        & 38.8 (\#11)           & 46.5 (\#10)             & \textbf{\textcolor{CBBlue}{91.3 (\#1)}}         & \textbf{\textcolor{CBOrange}{47.3 (\#12)}}          & \textbf{\textcolor{CBOrange}{94 (\#13)}}          & 52.6 (\#9)     & 38.2 (\#10)     \\ \hline
\footnotesize{(\textit{v})} PocketFinder\textsuperscript{+}\textsubscript{AA}    & 44.5 (\#6)          & 48.9 (\#8)            & 50.5 (\#7)        & 65.3 (\#11)           & 178 (\#11)         & 72.3 (\#2)     & 75.9 (\#2)     \\ \hline
\footnotesize{(\textit{v})} Ligsite\textsuperscript{+}\textsubscript{AA}         & 44.9 (\#5)          & 49.0 (\#7)              & 49.7 (\#9)        & 68.8 (\#9)          & 159 (\#12)         & \textbf{\textcolor{CBBlue}{77.6 (\#1)}}     & \textbf{\textcolor{CBBlue}{77.0 (\#1)}}     \\ \hline
\footnotesize{(\textit{v})} Surfnet\textsuperscript{+}\textsubscript{AA}         & 43.3 (\#9)          & 47.4 (\#9)            & 48.9 (\#10)        & 68.6 (\#10)          & 308 (\#10)         & 71.7 (\#3)     & 72.0 (\#3)     \\ \hline
\caption[Pocket level evaluation (\textit{best} variants)]{\textbf{Pocket level evaluation (\textit{best} variants).} Only the top-performing, i.e., highest top-\textit{N}+2 recall, variant of each method is present on this table. Recall (\%) for each method considering top-\textit{N}, \textit{N}+2 and \textit{all} predictions (max), i.e., maximum recall. Precision (\%) of the method for the top-1000 scored predictions. Number of TP reached for the first 100 FP (\# TP\textsubscript{100 FP}). Mean relative residue overlap (RRO) for correctly predicted sites and relative volume overlap (RVO) only for sites that have a volume, i.e., are pockets or cavities, and not exposed sites, which do not have a volume. RRO and RVO represent the overlap in residues and volume relative to the observed site. Bold font indicates the best (blue) and worst (orange) performing methods for each metric. (\textbf{d}) and (\textit{v}) indicate whether methods are default or variant, respectively.}
\label{tab:pocket_level_benchmark_variants}\\
\end{longtable}
\end{landscape}
\endgroup

\begin{landscape}
\begin{longtable}[c]{|c|c|c|c|c|c|c|}
\hline
\textbf{Default method (d)} & \textbf{Best variant (\textit{v})} & \textbf{\textDelta\textsubscript{Recall\textsubscript{top-\textit{N}}}} (\%) & \textbf{\textDelta\textsubscript{Recall\textsubscript{top-\textit{N}+2}}} (\%) & \textbf{\textDelta\textsubscript{Recall\textsubscript{max}}} (\%) & \textbf{\textDelta\textsubscript{Precision\textsubscript{1K}}} (\%) & \textbf{\textDelta\textsubscript{\# TP\textsubscript{100 FP}}} \\ \hline
\endfirsthead
%
\endhead
%
VN-EGNN                     & VN-EGNN\textsubscript{NR}              & +17.0                          & +5.2                           & \textminus3.0                        & \textminus1.0                          & \textminus273                     \\ \hline
IF-SitePred                 & IF-SitePred\textsubscript{RESC-NR}      & +9.9                         & +13.4                          & \textminus0.5                      & +3.3                         & +285                      \\ \hline
PUResNet                    & PUResNet\textsubscript{PRANK}          & +0.2                         & 0.0                             & 0.0                         & +11.7                        & +558                      \\ \hline
DeepPocket\textsubscript{SEG}              & DeepPocket\textsubscript{SEG-NR}       & +8.0                           & +5.6                           & \textminus1.1                      & \textminus1.0                          & \textminus27                      \\ \hline
PocketFinder\textsuperscript{+}                & PocketFinder\textsuperscript{+}\textsubscript{AA}         & +5.3                         & +1.1                           & 0.0                         & +23.3                        & +114                      \\ \hline
Ligsite\textsuperscript{+}                     & Ligsite\textsuperscript{+}\textsubscript{AA}              & +3.6                         & +0.6                           & 0.0                         & +16.5                        & +44                       \\ \hline
Surfnet\textsuperscript{+}                     & Surfnet\textsuperscript{+}\textsubscript{AA}              & +6.0                           & +1.6                           & 0.0                         & +29.1                        & +247                      \\ \hline
\caption[Methods improvement summary]{\textbf{Methods improvement summary.} Summary of the performance improvement for the seven methods for which non-redundant or re-scoring variants were generated in this Chapter: VN-EGNN, IF-SitePred, PUResNet, DeepPocket\textsubscript{SEG}, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}. Performance increase is calculated for each metric from the \textit{best} variant (\textit{v}), i.e., highest top-\textit{N}+2 recall, relative to the default original method (\textbf{d}). Maximum recall is reduced for VN-EGNN, IF-SitePred and DeepPocket\textsubscript{SEG} as their non-redundant variants present fewer pockets. The same happens for Precision\textsubscript{1K} and \# TP\textsubscript{100 FP} for VN-EGNN. Overall, the method variants introduced in this work have a significant positive effect on performance.
}
\label{tab:methods_improvement_summary}\\
\end{longtable}
\end{landscape}

\section{Discussion}

We have shown how redundancy in prediction, i.e., predicting multiple times the same observed site, can underestimate the recall, and overestimate the precision of the methods, therefore providing a misleading assessment of the methods' performance. Redundancy removal and subsequent pocket re-ranking can yield a significant increase in recall. The importance of a robust pocket scoring scheme can have a strong impact in the performance, both in recall and precision of the methods and emphasis should be put into this area. Even if a single site is predicted per protein, a pocket score can be highly useful when ranking pockets in different proteins, e.g., when having a list of potential drug targets and deciding which protein might be best to target therapeutically.

The performance improvements accomplished in this Chapter by removing redundant predictions (VN-EGNN, IF-SitePred and DeepPocket\textsubscript{SEG}) as well as using more sophisticated pocket scoring schemes (IF-SitePred, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}) are considerable, increasing recall by $>$15\%, precision by $\approx$30\% and \# TP\textsubscript{100 FP} by $>$500 \autoref{tab:methods_improvement_summary}. Nevertheless, the overall ranking of the methods including novel variants \autoref{tab:pocket_level_benchmark_variants} does not change much when comparing to the benchmark of the default methods \autoref{tab:pocket_level_benchmark}. This is due to the fact that the difference in Recall\textsubscript{top-\textit{N}+2} between the top-performing (fpocket and re-scored predictions) methods and the ones for which variants were generated (VN-EGNN, IF-SitePred and DeepPocket\textsubscript{SEG} was larger than the increase in recall resulting from the variants. While removing redundancy post-prediction has a significant improvement in performance (VN-EGNN\textsubscript{NR} and IF-SitePred\textsubscript{NR}), approaching this issue before prediction would be more beneficial. For VN-EGNN, which predicts a maximum of 8 sites, ensuring these 8 (or more) predictions are non-redundant is more desirable than removing redundant predictions ending up with 1/8 predictions. The same applies to IF-SitePred, where non-overlapping starting predictions are more convenient than dealing with redundancy post-prediction.

\section{Conclusions}

The conclusions resulting from the work presented in this Chapter are as follows:

\begin{itemize}

\item Redundancy in ligand binding site prediction leads to an underestimate of recall and an overestimate of precision. The removal of such redundancy and subsequent re-ranking of the remaining pockets results in a drastic increase in recall.

\item A robust pocket scoring scheme is crucial for the correct ranking and prioritisation of predicted sites in downstream analysis, e.g., docking, simulation. Additionally, it has a significant positive effect on both precision and recall.

\item IF-SitePred benefits significantly from pocket re-scoring, and suggests that protein embeddings, which are not directly dependent of structure, represent great promise in the field of ligand site prediction.

\end{itemize}

