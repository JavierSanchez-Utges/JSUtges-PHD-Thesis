\chapter{Improvement on methods for the prediction of protein-ligand binding sites}
\label{chap:LBS_IMPROV}

\section*{Preface}

This chapter explores in detail fifteen non-redundant and scoring variants of the thirteen methods evaluated in the previous chapter. The negative effect of poor scoring schemes and redundancy in ligand site prediction is demonstrated through the performance evaluation of these variants relative to their default modes using more than ten informative metrics.

\section*{Publications}

Utgés, J.S. and Barton, G.J. Comparative evaluation of methods for the prediction of protein-ligand binding sites. \textit{J Cheminform} \textbf{16}, 126 (2024). \url{https://doi.org/10.1186/s13321-024-00923-z}.

\section*{Author contributions}

J.S.U. and G.J.B. conceived, designed, and developed the research. J.S.U. analysed the data. J.S.U. developed the software. J.S.U. and G.J.B. wrote, reviewed and edited the manuscript. G.J.B. secured funding and supervised.

\section{Introduction}

Pocket prediction redundancy is defined here as the prediction of pockets with centroids very close in space ($D \leq$ 5\AA{}) or with overlapping residues ($JI \geq$ 0.75). This indicates multiple predictions of the same potential ligand binding site. Most ligand site prediction tools predict not only the location of the pocket by means of a centroid or pocket residues, but also a pocket confidence, and an associated rank among all the predicted pockets. Ligand site predictors tend to be evaluated by considering the top-$N$, or top-$N$+2 ranking pockets, where $N$ is the number of observed sites for a given protein. The redundant prediction of pockets can result in a sub-optimal ranking thus negatively affecting the performance of the predictors.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/ch_LBS_COMP/PNG/FIG6_PREDICTION_REDUNDANCY_SPLIT_1.png}
    \caption[The issue of redundancy in ligand binding site prediction]{\textbf{The issue of redundancy in ligand binding site prediction.} \textbf{(A)} A set of predictions where 6/10 (60\%) predictions are redundant, resulting in a low recall of 1/5 (20\%) and inflated precision of 7/7 (100\%); \textbf{(B)} When redundancy is removed, only four predictions remain and recall increases to 3/5 (60\%) and precision decreases to 3/4 (75\%).}
    \label{fig:prediction_redundancy}
\end{figure}

\FloatBarrier

\autoref{fig:prediction_redundancy} shows an example protein with $N$ = 5 observed pockets. A ligand site predictor returns 10 predictions, but the top-7 are all within 3 \AA{} of one of the observed pockets, and $>$12 \AA{} from any of the other four observed pockets. If the top-$N$+2 (top-7) predictions were considered, this would only recall a single unique pocket, as six of the top-7 predictions are redundant. Top-$N$+2 recall would then be 20\% (1/5). Precision, however, within this top-7 would be 100\% (7/7), as the seven predictions are correctly recalling an observed pocket (which happens to be the same). In this case, both the low recall and the high precision are artefacts resulting of the redundancy (\autorefpanel{fig:prediction_redundancy}{A}). Redundancy in prediction can often result in an overestimate of the precision and an underestimate of the recall. \autorefpanel{fig:prediction_redundancy}{B} illustrates what happens when redundant predictions are removed, keeping always higher-scoring predictions. When the six redundant predictions (blue stars) are removed, the other three predictions, which are of different pockets, are considered as now fall within the top-$N$+2 predictions. This increases the recall to 60\%, as 3/5 observed pockets are now correctly predicted. However, the precision decreases, as only three out of the four predictions made overlap with an observed pocket. Pocket rank \#2 has a high score but is not observed. This is a \textit{false positive} in this context, however it might be a candidate pocket yet to be resolved and could prove interesting as a drug target.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_COMP/PNG/FIG6_PREDICTION_REDUNDANCY_SPLIT_2.png}
    \caption[Example of redundant predictions]{\textbf{Example of redundant predictions.} Predictions by VN-EGNN, IF-SitePred and PUResNet, on chain D of PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/4z9m}{4Z9M} \cite{PDB_4Z9M} of human creatine kinase (\href{https://www.uniprot.org/uniprotkb/P17540/entry}{P17540}) where \href{https://www.ebi.ac.uk/pdbe-srv/pdbechem/chemicalCompound/show/ADP}{ADP} binds. For this \href{https://www.ebi.ac.uk/pdbe-srv/pdbechem/chemicalCompound/show/ADP}{ADP} binding site, VN-EGNN reports 7 predictions, IF-SitePred 33, and PUResNet a single prediction. These three methods correctly predict this site, however, VN-EGNN and IF-SitePred report redundant pocket predictions, which centroids are very close, $\leq$ 5 \AA{}, in space and residues overlap high ($\geq$ 0.75).}
    \label{fig:prediction_redundancy_examples}
\end{figure}

\autoref{fig:prediction_redundancy_examples} showcases PDB: \href{https://www.ebi.ac.uk/pdbe/entry/pdb/4z9m}{4Z9M} \cite{PDB_4Z9M} of human creatine kinase S-type, mitochondrial (\href{https://www.uniprot.org/uniprotkb/P17540/entry}{P17540}) as an example of this phenomenon, where VN-EGNN and IF-SitePred redundantly predict the same pocket 7 and 33 times, whereas PUResNet returns a single prediction. All three methods correctly predict the site, just the difference is in the number of returned predictions.

\section{Methods}

\subsection{Generation of ``non-redundant`` sets of predictions}

\autorefpanel{fig:pocket_features_2}{B} shows that prediction redundancy is an issue particularly for VN-EGNN, IF-SitePred, and to a lesser extent DeepPocket\textsubscript{SEG}. To assess the effect that redundancy has on the performance of these methods, non-redundant subsets of predictions were obtained and labelled with the subscript ``NR''. A predicted pocket $i$ is considered redundant if there exists a pocket $j \neq i$ so that the distance between their centroids $D_{i,j} \leq$ 5 \AA{} or their residue overlap $JI_{i,j} >$ 0.75, i.e., they share at least 3/4 (75\%) of their residues. Refer to \autoref{fig:closest_pred_pockets} for the closest predicted sites for each method. Redundancy filtering was carried out for each method keeping always the higher scoring pocket. Redundancy (\%) was calculated as the proportion of redundant pockets relative to the original total number of pockets. VN-EGNN presents the highest percentage of redundant pockets with 9066/13,582 (67\%) redundant pockets, followed by IF-SitePred with 22,232/44,948 (49\%), and DeepPocket\textsubscript{SEG} with 6744/21,718 (31\%). For other methods, redundancy was minimal ($<$1\%).

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG9_CLOSEST_PREDICTED_POCKETS.png}
    \caption[Closest predicted pockets for each methods]{\textbf{Closest predicted pockets for each method.} LIGYSIS is a reference dataset, not a prediction method. For each method, the two closest predicted pockets across all protein chains are shown. This is the pair of pockets with the minimum Euclidean distance between their centroids. Protein surface is coloured in tan. The larger pocket (more residues) and centroid is coloured in the method colour, and the other in grey. A distance threshold of $D$ = 5 \AA{} was selected to determine whether a pocket prediction was redundant. VN-EGNN, IF-SitePred and DeepPocket\textsubscript{SEG} clearly differ from other methods presenting distances $<$ 1 \AA{}.}
    \label{fig:closest_pred_pockets}
\end{figure}

\subsection{Pocket re-scoring strategies}

PUResNet, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+} do not score, nor explicitly rank their pockets, and so they were taken in the order given by their pocket ID. This means that when sorting across the dataset, the order of all pockets with the same rank is arbitrary. To obtain a score for these pockets, multiple strategies were employed. Firstly, a pocket score was obtained as the number of pocket amino acids, resulting in variants PUResNet\textsubscript{AA}, PocketFinder\textsuperscript{+}\textsubscript{AA}, Ligsite\textsuperscript{+}\textsubscript{AA} and Surfnet\textsuperscript{+}\textsubscript{AA}. Secondly, PRANK pocket scoring was employed, resulting in variants PUResNet\textsubscript{PRANK}, PocketFinder\textsuperscript{+}\textsubscript{PRANK}, Ligsite\textsuperscript{+}\textsubscript{PRANK} and Surfnet\textsuperscript{+}\textsubscript{PRANK}. IF-SitePred uses a simple pocket scoring scheme, whtih assigns to each centroid the number of clustered cloud points it results from. In this chapter, newly defined IF-SitePred pocket scores were calculated were calculated as the sum of squares (SS) of the ligandability scores ($LS_{i}$), calculated with \autoref{eq:IFSP_score}, of the $K$ residues on a site (\autoref{eq:IFSP_pocket_score}) resulting in IF-SitePred\textsubscript{RESC}. For PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+}, and Surfnet\textsuperscript{+} the same was done but instead of residue scores, grid point scores ($GS_{i}$) were used (\autoref{eq:leagcy_methos_pocket_score}). This resulted in further variants PocketFinder\textsuperscript{+}\textsubscript{SS}, Ligsite\textsuperscript{+}\textsubscript{SS}, and Surfnet\textsuperscript{+}\textsubscript{SS}. This is the same approach introduced by Krivák, \textit{et al.} \cite{KRIVAK_2015_P2RANK} and later adopted by Smith \textit{et al.} \cite{SMITH_2024_GrASP}.

\begin{equation}
SS_{\text{IF--SitePred}} = \sum_{i=1}^{K} LS_i^2
\label{eq:IFSP_pocket_score}
\end{equation}

\begin{equation}
SS_{\text{PocketFinder}^+} = SS_{\text{Ligsite}^+} = SS_{\text{Surfnet}^+} = \sum_{i=1}^{K} GS_i^2
\label{eq:leagcy_methos_pocket_score}
\end{equation}

\subsection{Performance evaluation}

\FloatBarrier

\section{Results}

\subsection{Effect of redundancy and pocket score on ranking}

Because of $K$ = 8 virtual nodes are used in the default VN-EGNN implementation, a maximum of $N$ = 8 predicted pockets are possible. However, only seven are observed in our dataset, i.e., in all cases at least one virtual node gets clustered with another, resulting in 7 ``unique'' predictions. \autorefpanel{fig:pocket_score_vs_rank1}{A} illustrates the issue of redundancy in pocket predictions and how it affects the scoring and ranking of the pockets. A prediction of the same pocket is reported multiple times as distinct virtual nodes, or pocket centroids, which are very close to each other, and present very similar scores. This is why there is no apparent difference in the distribution of scores across the pocker ranks for VN-EGNN, unlike all other methods. After removing redundancy and obtaining VN-EGNN\textsubscript{NR}, this is no longer the case (\autorefpanel{fig:pocket_score_vs_rank1}{B}).

\begin{figure}[htb!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG3_SCORE_vs_RANK_SPLIT1.png}
    \caption[Pocket score \textit{vs} pocket ranking]{\textbf{Pocket score \textit{vs} pocket ranking.} \textbf{(A)} VN-EGNN reported pocket scores; \textbf{(B)} Non-redundant VN-EGNN predictions (VN-EGNN\textsubscript{NR}); \textbf{(C)} Default IF-SitePred predictions are ranked based on the number of pocket cloud points; \textbf{(D)} Non-redundant variant of IF-SitePred (IF-SitePred\textsubscript{NR}); \textbf{(E)} Re-scored non-redundant IF-SitePred predictions (IF-SitePred\textsubscript{RESC-NR}). Score is calculated as sum of squares of residue ligandability scores (\autoref{eq:IFSP_pocket_score}); \textbf{(F)} GrASP; \textbf{(G)} PUResNet does not score its pockets. PUResNet\textsubscript{AA}. This variant uses the number of pocket amino acids as a score; \textbf{(H)} PRANK scored PUResNet pockets; \textbf{(I)} DeepPocket\textsubscript{SEG}; \textbf{(J)} Non-redundant DeepPocket\textsubscript{SEG} predictions (DeepPocket\textsubscript{SEG-NR}); \textbf{(K)} DeepPocket\textsubscript{RESC}; \textbf{(L)} P2Rank\textsubscript{CONS}. (\textbf{d}) and (\textit{v}) indicate whether methods are default, or a variant generated in this work.}
    \label{fig:pocket_score_vs_rank1}
\end{figure}

IF-SitePred predictions are also highly redundant. However, these predictions, despite being close to each other, will present different scores (number of points), that is why higher ranks (1, 2, 3...) present higher scores (\autorefpanel{fig:pocket_score_vs_rank1}{C}). Redundancy removal can be observed in \autorefpanel{fig:pocket_score_vs_rank1}{D} as the scatter plot is less crowded and the maximum rank across the dataset is 60, as opposed to 120. \autorefpanel{fig:pocket_score_vs_rank1}{E} shows the non-redundant set of re-scored IF-SitePred predictions, IF-SitePred\textsubscript{RESC-NR}. This score distribution is wider, i.e., scores take values from a larger distribution of values, which might yield a more relevant scoring of pockets.

There is no clear difference between \autorefpanel{fig:pocket_score_vs_rank1}{G-H}, meaning that using PRANK to score PUResNet predictions does not alter the overall ranking of the predictions made within a protein. This makes sense, as only 10\% of proteins present $>$1 predicted pocket. This new score, however, could help in the ranking of pockets across the dataset, and not just within a protein.

The distribution of scores does not change when removing the redundancy from DeepPocket\textsubscript{SEG} predictions (\autorefpanel{fig:pocket_score_vs_rank1}{I-J}), but the maximum rank goes from 200 to 140 indicating the decrease in total predictions. The score distributions of fpocket\textsubscript{PRANK} (\autorefpanel{fig:pocket_score_vs_rank2}{B}) and fpocket (\autorefpanel{fig:pocket_score_vs_rank2}{C}) are completely different which means the ranking of pockets, and therefore recall and precision might differ considerably between these two scoring schemes of the same predictions.

The score distributions of ``\textsubscript{AA}'', ``\textsubscript{SS}'' and ``\textsubscript{PRANK}'' variants of PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+} are similar, suggesting that the number of pocket amino acids might dictate the order in which these pockets are reported \autorefpanel{fig:pocket_score_vs_rank2}{D-L} and that re-scoring the predictions by these methods might not have an effect on their performance.

%\begin{figure}[ht!]
%    \centering
%    \includegraphics[width=\textwidth]{figures/ch_LBS_COMP/OLD/SUPP/PNG/SUPP_FIG3_SCORE_vs_RANK.png}
%    \caption[Pocket score \textit{vs} pocket ranking]{\textbf{Pocket score \textit{vs} pocket ranking.}}
%    \label{fig:pocket_score_vs_rank}
%\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG3_SCORE_vs_RANK_SPLIT2.png}
    \caption[Pocket score \textit{vs} pocket ranking]{\textbf{Pocket score \textit{vs} pocket ranking.} \textbf{(A)} P2Rank; \textbf{(B)} fpocket\textsubscript{PRANK}; \textbf{(C)} fpocket. This distribution differs massively from the re-scored fpocket\textsubscript{PRANK} one; \textbf{(D)} PocketFinder\textsuperscript{+} does not report pocket scores, so the number of pocket residues is displayed for the PocketFinder\textsuperscript{+}\textsubscript{AA} variant; \textbf{(E)} PocketFinder\textsuperscript{+}\textsubscript{PRANK}; \textbf{(F)} PocketFinder\textsuperscript{+}\textsubscript{SS}. This variant uses the pocket grid points’ scores to calculate a pocket score by summing the squared scores (\autoref{eq:leagcy_methos_pocket_score}); \textbf{(G)} Just like PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} does not score pockets, Y-axis is number of pocket residues (Ligsite\textsuperscript{+}\textsubscript{AA}); \textbf{(H)} Ligsite\textsuperscript{+}\textsubscript{PRANK}; \textbf{(I)} Ligsite\textsuperscript{+}\textsubscript{SS}; \textbf{(J)} Surfnet\textsubscript{+}\textsubscript{AA}; \textbf{(K)} Surfnet\textsuperscript{+}\textsubscript{PRANK}; \textbf{(L)} Surfnet\textsuperscript{+}\textsubscript{SS}. (\textbf{d}) and (\textit{v}) indicate whether methods are default, or a variant generated in this work.}
    \label{fig:pocket_score_vs_rank2}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG5_RECALL_VARIANTS_1.png}
    \caption[Recall curves for method variants (I)]{\textbf{Recall curves for method variants (I).} Recall curves for different scoring and ranking variants for VN-EGNN \textbf{(A-C)}, IF-SitePred \textbf{(D-F)}, PUResNet \textbf{(G-I)} and DeepPocket\textsubscript{SEG} \textbf{(J-L)}. For each method, panels illustrate how recall changes as DCC, rank and \textit{I\textsubscript{rel}} thresholds vary. In this last one \textit{I\textsubscript{rel}} is the criterion used to classify predictions. Dashed lines indicate the thresholds used as reference in this work: DCC = 12 \AA{}, rank = top-\textit{N}+2, and \textit{I\textsubscript{rel}} = 0.5. (\textbf{d}) and (\textit{v}) indicate whether methods are default, or variants.}
    \label{fig:pocker_recall_variants1}
\end{figure}

\FloatBarrier

\subsection{Effect of redundancy and pocket score on recall}

\autoref{fig:pocket_score_vs_rank1} and \autoref{fig:pocket_score_vs_rank2} demonstrate how removing redundancy from predictions can have a drastic effect in the ranking of the predictions, with VN-EGNN being the clearest example. The following analysis explores the effect that redundancy removal and different pocket scoring schemes might have on recall for PUResNet, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}, which do not report pocket scores.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG5_RECALL_VARIANTS_2.png}
    \caption[Recall curves for method variants (II)]{\textbf{Recall curves for method variants (II).} Recall curves for different scoring and ranking variants for PocketFinder\textsuperscript{+} \textbf{(A-C)}, Ligsite\textsuperscript{+} \textbf{(D-F)} and Surfnet\textsuperscript{+} \textbf{(G-I)}. For each method, panels illustrate how recall changes as DCC, rank and \textit{I\textsubscript{rel}} thresholds vary. In this last one \textit{I\textsubscript{rel}} is the criterion used to classify predictions. Dashed lines indicate the thresholds used as reference in this work: DCC = 12 \AA{}, rank = top-\textit{N}+2, and \textit{I\textsubscript{rel}} = 0.5. (\textbf{d}) and (\textit{v}) indicate whether methods are default, or variants.}
    \label{fig:pocker_recall_variants2}
\end{figure}

\autorefpanel{fig:pocker_recall_variants1}{A-B} shows a significant +5.2\% increase in recall after removing redundancy for VN-EGNN predictions (Recall = 46.1\%). This increase corresponds to 346 extra predictions that fall within the top-$N$+2 after removing redundancy. An even stronger improvement can be observed for IF-SitePred (\autorefpanel{fig:pocker_recall_variants1}{C-D}), where a combination of redundancy removal and pocket re-scoring (\autoref{eq:IFSP_pocket_score}) results in a significant increase of +13.4\% (Recall = 39.1\%), corresponding to 901 added predictions falling in the considered top-$N$+2 predictions. Most of this change is due to the redundancy removal, as can be seen by the higher recall of IF-SitePred\textsubscript{NR}. Scoring of PUResNet predictions using the number of pocket amino acids (PUResNet\textsubscript{AA}) or PRANK (PUResNet\textsubscript{PRANK}) had no effect on the recall. This was expected as PUResNet predicts a single pocket in 90\% of the cases, and therefore, there is no strong need for a score to sort predictions within a protein. (\autorefpanel{fig:pocker_recall_variants1}{G-H}). Just like VN-EGNN and IF-SitePred, the recall of DeepPocket\textsubscript{SEG} benefits from redundancy removal, increasing by +5.6\% (\autorefpanel{fig:pocker_recall_variants1}{J-K}). For PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}, none of the variants had a significant improvement in the recall (\autorefpanel{fig:pocker_recall_variants2}{A-I}). This is expected as these predict only a few non-redundant sites per protein, with medians ranging 1-3 pockets per protein. These pockets, might already be sorted by number of amino acids as suggested by \autorefpanel{fig:pocket_score_vs_rank2}{D-L}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG6_TP100FP_VARIANTS.png}
    \caption[ROC100 curves for non-redundant and re-scored variants]{\textbf{ROC100 curves for non-redundant and re-scored variants.} For each method, predicted pockets across the whole dataset, i.e., all LIGYSIS protein chains, are ranked by their score. This way, pockets with the highest scores will be at the top of the list, whereas pockets with the lowest scores will be at the bottom. This ranking will not correspond to ranking pockets across the dataset by their rank, as a pocket ranked \#2, \#3 or lower could have a higher score than a pocket \#1 of a different protein. Each method has a colour assigned, and each variant resulting of redundancy removal or pocket (re-)scoring a different line style. \textbf{(A)} VN-EGNN and ``\textsubscript{NR}'' variant; \textbf{(B)} IF-SitePred and ``\textsubscript{NR}'', re-scored ``\textsubscript{RESC}'' and ``\textsubscript{RESC-NR}'' variants; \textbf{(C)} PUResNet and ``\textsubscript{AA}'' and ``\textsubscript{PRANK}'' variants; \textbf{(D)} DeepPocket\textsubscript{SEG} and ``\textsubscript{NR}'' variant; \textbf{(E)} PocketFinder\textsuperscript{+} and ``\textsubscript{AA}'', ``\textsubscript{PRANK}'' and ``\textsubscript{SS}'' variants; \textbf{(F)} Ligsite\textsuperscript{+} and variants; \textbf{(G)} Surfnet\textsuperscript{+} and variants.}
    \label{fig:pocket_ROC100_variants}
\end{figure}

\FloatBarrier

\subsection{Effect of redundancy and pocket score on \# TP\textsubscript{100 FP}}

There are no negative predictions, either true (TN) or false (FN) in the context of ligand binding site prediction at the pocket level and accordingly, standard ROC/AUC curves cannot be obtained. Only positives are predicted (pockets). FN can be obtained by examining the observed pockets that are not predicted, but there are not scores for them. ROC100 curves provide an alternative to observe the relationship between true (TP) and false positives (FP). Predictions for each method across the whole reference dataset, LIGYSIS, were sorted based on pocket score and cumulative TP and FPs were counted until a certain number of FP was reached, in this case, 100. This visualisation provides insight into how well high-scoring predictions match the ground truth. A higher number of TP at FP = 100 indicates that the high scoring pockets recapitulate well the ground truth, whereas a low number indicates that the high scoring pockets do not match with the observed data, given the used threshold of DCC $\leq$ 12 \AA{}. It is important to understand that FPs in this context might not represent wrong predictions, but could be binding sites that are not considered in our ground truth dataset, that is comprised by biologically relevant protein-ligand interactions as defined in BioLiP, or relevant sites that simply have not been experimentally determined yet. It is also important to contextualise this metric with success rate, or recall, i.e., how many of the observed sites are predicted by each method given the above-mentioned threshold, as well as a rank threshold: top-$N$+2. A method might present a high number of TP within the first 100 FP yet have a low recall overall. \autoref{fig:pocket_ROC100_variants} explores how ROC100 changes for the non-redundant ``\textsubscript{NR}'' and re-scored ``\textsubscript{AA}'', ``\textsubscript{PRANK}'', ``\textsubscript{SS}'', and ``\textsubscript{RESC}'' sets of VN-EGNN, IF-SitePred, PUResNet, DeepPocket\textsubscript{SEG}, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}.

\autorefpanel{fig:pocket_ROC100_variants}{A} illustrates how redundancy can be misleading and overestimate the performance of VN-EGNN. Removing redundancy results in $\Delta_{TP}$ = \textminus273 (TP = 1028). This is because redundant predictions by VN-EGNN are very close in space and present very similar scores \autorefpanel{fig:pocket_score_vs_rank1}{A}. Because of this, in the redundant default set of predictions, multiple TP counts are being added for predictions of the same observed pocket. Even with redundancy removed, VN-EGNN reached 1028 TP for the first 100FP, indicating that the non-redundant higher scoring pockets recapitulate well the observed data.

There is no difference between IF-SitePred and IF-SitePred\textsubscript{NR} (curves overlap completely), which indicates that despite the redundancy in predictions by this method, its scoring scheme can sort sites in a meaningful manner. Considering multiple proteins with redundant predictions for IF-SitePred: the scoring scheme allows for the top-1 site of each of these proteins to rank above any of the other redundant predictions of the other proteins. The re-scored and non-redundant set of IF-SitePred predictions, IF-SitePred\textsubscript{RESC-NR}, results in a $\Delta_{TP}$ = +285 (TP = 1246), indicating that IF-SitePred could benefit from a more sophisticated scoring scheme, rather than the number of cloud points per binding site (\autorefpanel{fig:pocket_ROC100_variants}{B}).

\autorefpanel{fig:pocket_ROC100_variants}{C} is a perfect example of the importance of scoring pocket predictions. PUResNet does not score its predictions. For this reason, within a protein, pockets have been ranked based on the order they are reported, i.e., on their identifier. When sorting across the whole dataset, pockets with the same ID or rank where randomly shuffled. A massive increase in TP could be observed when simply sorting by the number of pocket residues and using PRANK to score this pockets provides an even larger increment in TP ($\Delta_{TP}$ = +563) (TP = 1097). An application of this could be running PUResNet on a list of potential drug target proteins. It would add great value to be able to rank the predictions among the targets to decide on a target.

The curve does not change much for DeepPocket\textsubscript{SEG}, ($\Delta_{TP}$ = \textminus27) (TP = 643), indicating that despite the segmentation module of DeepPocket might result in overlapping pockets, their scoring scheme is robust. It is important to consider that the pocket score results from re-scoring the fpocket candidates, which are not redundant. The redundancy in DeepPocket\textsubscript{SEG} is therefore unrelated to their scoring scheme. These results suggest that there is a big difference between fpocket candidates and extracted DeepPocket pockets and it might not be appropriate to consider the score of the former for the latter (\autorefpanel{fig:pocket_ROC100_variants}{D}).

For the last three methods, earlier and geometry/energy-based, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}, the results agree in that simply using the number of pocket amino acids results in the maximum TP for 100 FP: $\Delta_{TP}$ = +114 (TP = 178) (\autorefpanel{fig:pocket_ROC100_variants}{F}), $\Delta_{TP}$ = +44 (TP = 159) (\autorefpanel{fig:pocket_ROC100_variants}{G}) and $\Delta_{TP}$ = +247 (TP = 308) (\autorefpanel{fig:pocket_ROC100_variants}{H}). This is surprising, as sum of squares ``\textsubscript{SS}'' and ``\textsubscript{PRANK}'' scoring schemes have worked better for other methods. This result might be related to the fact that pockets predicted by these three methods tend to be larger than those predicted by other methods.

\subsection{Effect of redundancy and pocket score on precision}

For the same reason as why ROC/AUC curves cannot be calculated for ligand binding site prediction (at the pocket level), precision-recall (PR)/AUC curves cannot be either, as false negatives (FN) are not predicted, and therefore not scored. Nevertheless, precision, as the ratio of TP/TP+FP, can be measured. For this, as it was done for ROC100, all predictions for a method were sorted by pocket score and precision calculated as more predictions are considered.

\autoref{fig:pocket_precision_variants} portrays the precision curve for the top-1000 predictions for the non-redundant and re-scored variants for VN-EGNN, IF-SitePred, PUResNet, DeepPocket\textsubscript{SEG}, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}.

There is no significant ($p >$ 0.05) change in precision between VN-EGNN and VN-EGNN\textsubscript{NR} within the first 1000 predictions, precision = 91.5\% (\autorefpanel{fig:pocket_precision_variants}{A}). The same can be said for IF-SitePred with a precision = 94.3\% (\autorefpanel{fig:pocket_precision_variants}{B}). Using PRANK to score PUResNet pockets results in a significant +11.7\% increase in precision of the top-1000 predictions (precision = 93.3\%) (\autorefpanel{fig:pocket_precision_variants}{C}). DeepPocket\textsubscript{SEG-NR}, as the other redundant methods, does not experience a significant change in precision as redundancy is removed (precision = 81.6\%) (\autorefpanel{fig:pocket_precision_variants}{D}). For PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}, using the number of pocket amino acids results, ``\textsubscript{AA}'', in a precision increase of +23.3\% (precision = 65.3\%), (\autorefpanel{fig:pocket_precision_variants}{E}), +16.5\% (precision = 68.8\%) (\autorefpanel{fig:pocket_precision_variants}{F}) and 29.1\% (precision = 68.8\%) (\autorefpanel{fig:pocket_precision_variants}{G}), respectively. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG7_PRECISION_VARIANTS.png}
    \caption[Precision\textsubscript{1K} curves for non-redundant and re-scored variants]{\textbf{Precision\textsubscript{1K} curves for non-redundant and re-scored variants.}  Precision\textsubscript{1K} represents the precision (\%) calculated for the top-scoring 1000 predictions. For each method, predicted pockets across the whole LIGYSIS set are ranked by their score. This way, pockets with the highest scores are at the top of the list, whereas pockets with the lowest scores at the bottom. Each method has a colour assigned, and each scoring variant its own line style. $\Delta_{Precision}$ indicates the difference in precision between the selected method variant and the default one.  \textbf{(A)} VN-EGNN and ``\textsubscript{NR}'' variant; \textbf{(B)} IF-SitePred and ``\textsubscript{NR}'', re-scored ``\textsubscript{RESC}'' and ``\textsubscript{RESC-NR}'' variants; \textbf{(C)} PUResNet and ``\textsubscript{AA}'' and ``\textsubscript{PRANK}'' variants; \textbf{(D)} DeepPocket\textsubscript{SEG} and ``\textsubscript{NR}'' variant; \textbf{(E)} PocketFinder\textsuperscript{+} and ``\textsubscript{AA}'', ``\textsubscript{PRANK}'' and ``\textsubscript{SS}'' variants; \textbf{(F)} Ligsite\textsuperscript{+} and variants; \textbf{(G)} Surfnet\textsuperscript{+} and variants. Error bars indicate 95\% CI of the precision (proportion) and are displayed every 100 predictions.}
    \label{fig:pocket_precision_variants}
\end{figure}

\FloatBarrier

\subsection{Evaluation of predictive performance}



\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_IMPROV/PNG/SUPP_FIG8_POCKET_LEVEL_BENCHMARK_IMPROVED.png}
    \caption[Ligand binding site prediction at the pocket level (variants)]{\textbf{Ligand binding site prediction at the pocket level (variants).} Only the top-performing, i.e., highest top-\textit{N}+2 recall, variant of each method is drawn on this figure, e.g., IF-SitePred\textsubscript{RESC-NR} or VN-EGNN\textsubscript{NR} instead of their default modes. \textbf{(A)} Top-\textit{N}+2 recall, percentage of observed sites that are correctly predicted by a method according to a DCC threshold. Reported recall on \autoref{tab:pocket_level_benchmark_variants} corresponds to DCC = 12 \AA{}; \textbf{(B)} Recall using DCC = 12 \AA{} but considering increasing rank thresholds. \textit{ALL} represents the maximum recall of a method, obtained by considering all predictions, regardless of their rank or score; \textbf{(C)} Recall curve using \textit{I\textsubscript{rel}} as a criterion; \textbf{(D)} ROC100 curve (cumulative TP against cumulative FP until 100 FP are reached); \textbf{(E)} Precision curve for the top-1000 predictions of each method across the LIGYSIS dataset. Error bars represent 95\% CI of the recall \textbf{(A-C)} and precision \textbf{(E)}, which are 100 $\times$ proportion. Numbers at the right of the panels indicate groups or blocks of methods that perform similarly for each metric. Stars ``*'' indicate outlier methods, or methods that perform very differently than the rest.}
    \label{fig:pocket_level_benchmark_variants}
\end{figure}


\begin{landscape}
\begin{longtable}[c]{|M{33mm}|M{27mm}|M{29mm}|M{24mm}|M{27mm}|M{21mm}|M{19mm}|M{19mm}|}
\hline
\textbf{Method}         & \textbf{\% Recall\textsubscript{top-$N$}} & \textbf{\% Recall\textsubscript{top-$N$+2}} & \textbf{\% Recall\textsubscript{max}} & \textbf{\% Precision\textsubscript{1K}} & \textbf{\# TP\textsubscript{100 FP}} & \textbf{\% $RRO$} & \textbf{\% $RVO$} \\ \hline
\endfirsthead
%
\multicolumn{8}{c}%
{{\bfseries Table \thetable\ continued from previous page}} \\
\hline
\textbf{Method}         & \textbf{\% Recall\textsubscript{top-$N$}} & \textbf{\% Recall\textsubscript{top-$N$+2}} & \textbf{\% Recall\textsubscript{max}} & \textbf{\% Precision\textsubscript{1K}} & \textbf{\# TP\textsubscript{100 FP}} & \textbf{\% $RRO$} & \textbf{\% $RVO$} \\ \hline
\endhead
%
VN-EGNN\textsubscript{NR}          & 44.5           & 46.1             & 46.3         & 91.5           & 1,028       & \textbf{\textcolor{firebrick}{27}}     & \textbf{\textcolor{firebrick}{19}}     \\ \hline
IF-SitePred\textsubscript{RESC-NR} & \textbf{\textcolor{firebrick}{29.7}}           & \textbf{\textcolor{firebrick}{39.1}}             & 51           & \textbf{\textcolor{forestgreen}{94.3}}           & \textbf{\textcolor{forestgreen}{1,246}}       & 47     & 37     \\ \hline
GrASP              & 48             & 49.9             & 50           & 92.5           & 1,017       & 56     & 69     \\ \hline
PUResNet\textsubscript{PRANK}      & 40.8           & 41.1             & \textbf{\textcolor{firebrick}{41.1}}         & 93.3           & 1,097       & 65     & 73     \\ \hline
DeepPocket\textsubscript{SEG-NR}   & 43.4           & 49.4             & 55.4         & 81.6           & 643         & 61     & 73     \\ \hline
DeepPocket\textsubscript{RESC}     & 46.6           & 58.1             & 89.3         & 81.7           & 637         & 50     & 41     \\ \hline
P2Rank\textsubscript{CONS}         & 48.4           & 53.9             & 57           & 90.7           & 932         & 57     & 50     \\ \hline
P2Rank             & 46.7           & 51.9             & 57           & 79.2           & 586         & 56     & 66   \\ \hline
fpocket\textsubscript{PRANK}       & \textbf{\textcolor{forestgreen}{48.8}}           & \textbf{\textcolor{forestgreen}{60.4}} & \textbf{\textcolor{forestgreen}{91.3}}         & 81.7           & 526         & 59     & 41     \\ \hline
PocketFinder\textsuperscript{+}\textsubscript{AA}    & 44.5           & 48.9             & 50.5         & \textbf{\textcolor{firebrick}{65.3}}           & 178         & 79     & 97     \\ \hline
Ligsite\textsuperscript{+}\textsubscript{AA}         & 44.9           & 49               & 49.7         & 68.8           & \textbf{\textcolor{firebrick}{159}}         & \textbf{\textcolor{forestgreen}{88}}     & \textbf{\textcolor{forestgreen}{98}}     \\ \hline
Surfnet\textsuperscript{+}\textsubscript{AA}         & 43.3           & 47.3             & 48.9         & 68.6           & 308         & 76     & 92     \\ \hline
\caption[Pocket level evaluation (\textit{best} variants)]{\textbf{Pocket level evaluation (\textit{best} variants).} Only the top-performing, i.e., highest recall, variant of each method is present on this table, e.g., IF-SitePred\textsubscript{RESC-NR} or VN-EGNN\textsubscript{NR}  instead of their default modes. Recall (\%) for each method considering top-$N$, $N$+2 and \textit{all} predictions (max) without taking rank into consideration, i.e., maximum recall. Precision (\%) of the method for the top-1000 scored predictions. Number of TP reached for the first 100 FP (\# TP\textsubscript{100 FP}). Mean relative residue overlap (RRO) for those sites correctly predicted and relative volume overlap (RVO) only for correctly predicted sites that have a volume, i.e., are pockets or cavities, and not exposed sites, which do not have a volume. These last two metrics represent the overlap in residues and volume relative to the observed site. Bold font indicates the best (green) and worst (red) performing methods for each metric.}
\label{tab:pocket_level_benchmark_variants}\\
\end{longtable}
\end{landscape}

\section{Discussion}

We have shown how redundancy in prediction, i.e., predicting multiple times the same observed site, can underestimate the recall, and overestimate the precision of the methods, therefore providing a misleading assessment of the methods’ performance. Redundancy removal and subsequent pocket re-ranking can yield a significant increase in recall. The importance of a robust pocket scoring scheme can have a strong impact in the performance, both in recall and precision of the methods and emphasis should be put into this area. Even if a single site is predicted per protein, a pocket score can be highly useful when ranking pockets in different proteins, e.g., when having a list of potential drug targets and deciding which protein might be best to target therapeutically.

fpocket\textsubscript{PRANK} (60\%) and DeepPocket\textsubscript{RESC} (58\%) present the highest recall of the methods reviewed in this work. P2Rank\textsubscript{CONS} and P2Rank follow closely with 54\% and 52\% recall, then GrASP (50\%), DeepPocket\textsubscript{SEG-NR}, Ligsite\textsuperscript{+}\textsubscript{AA} and PocketFinder\textsuperscript{+}\textsubscript{AA} with 49\%, Surfnet\textsuperscript{+}\textsubscript{AA} (47\%), VN-EGNNNR (46\%), PUResNet\textsubscript{PRANK} (41\%) and IF-SitePred\textsubscript{RESC-NR} (39\%). fpocket is the method that predicts the most pockets per protein, reaching a maximum recall between 80-90\% (considering all pockets regardless of the rank). P2Rank\textsubscript{CONS} comes second with a maximum recall of 50-60\%. The rest of the methods range 40-55\%. This indicates that whilst there are still some pockets un-predicted by fpocket (10-20\%), the maximum recall of this method is 20-30\% higher than any other method. However, considering top-$N$+2 pockets, fpocket only recalls 47\% of the observed pockets. fpocket\textsubscript{PRANK} and DeepPocket\textsubscript{RESC} gain $>$10\% in recall by simply re-scoring those predictions. This highlights the paramount importance of a robust scoring scheme, which captures well the nature of binding sites and places those with a higher probability of being real binding sites at the top of the ranking. Newer methods like VN-EGNN, IF-SitePred, GrASP and PUResNet are the most precise methods, however because of redundancy in predictions (VN-EGNN, IF-SitePred), or low number of predicted pockets per protein (VN-EGNN, GrASP and PUResNet) are limited in their recall. Their high precision indicates that their models learn and capture well the nature of ligand binding sites and so they represent a great venue to pursue in the field of ligand binding site prediction. Whilst removing redundancy post-prediction has a significant improvement in performance (VN-EGNN\textsubscript{NR} and IF-SitePred\textsubscript{NR}), approaching this issue before prediction would be more beneficial. For VN-EGNN, which predicts a maximum of 8 sites, ensuring these 8 (or more) predictions are non-redundant is more desirable than removing redundant predictions ending up with 1/8 predictions. The same applies to IF-SitePred, where non-overlapping starting predictions are more convenient than dealing with redundancy post-prediction.

\section{Conclusions}

The conclusions resulting from the work presented in this chapter are as follows:

\begin{itemize}

\item Redundancy in ligand binding site prediction leads to an underestimate of recall and an overestimate of precision. The removal of such redundancy and subsequent re-ranking of the remaining pockets results in a drastic increase in recall.

\item A robust pocket scoring scheme is crucial for the correct ranking and prioritisation of predicted sites in downstream analysis, e.g., docking, simulation. Additionally, it has a significant positive effect on both precision and recall.

\item Re-scoring of fpocket predictions, as fpocket\textsubscript{PRANK} or DeepPocket\textsubscript{RESC} present the highest recall (60\%) among the methods reviewed in this analysis.

\item IF-SitePred benefits significantly from pocket re-scoring, and suggests that protein embeddings, which are not directly dependent of structure, represent great promise in the field of ligand site prediction.

\end{itemize}

