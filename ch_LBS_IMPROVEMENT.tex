\chapter{Improvement on methods for the prediction of protein-ligand binding sites}

\section*{Preface}

XXX.

\section*{Publications}

Utgés J.S. and Barton G.J. Comparative evaluation of methods for the prediction of protein-ligand binding sites. \textit{J Cheminform} \textbf{16}, 126 (2024). \url{https://doi.org/10.1186/s13321-024-00923-z}. \cite{UTGES_2024_LBSCOMP}

\section*{Author contributions}

J.S.U. and G.J.B. conceived, designed, and developed the research. J.S.U. analysed the data. J.S.U. developed the software. J.S.U. and G.J.B. wrote, reviewed and edited the manuscript. G.J.B. secured funding and supervised.

\section{Introduction}

%\subsection{Redundancy in ligand site prediction}

We define pocket prediction redundancy as the prediction of pockets with centroids very close in space ($D \leq$ 5\AA{}) or with overlapping residues ($JI \geq$ 0.75). This indicates multiple predictions of the same potential ligand binding site. Most ligand site prediction tools predict not only the location of the pocket by means of a centroid or pocket residues, but also a pocket confidence, and an associated rank among all the predicted pockets. Ligand site predictors tend to be evaluated by considering the top-$N$, or top-$N$+2 ranking pockets, where $N$ is the number of observed sites for a given protein. The redundant prediction of pockets will result in a sub-optimal ranking and therefore affect negatively the performance of the predictors.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/ch_LBS_COMP/MAIN/PNG/FIG6_PREDICTION_REDUNDANCY_SPLIT_1.png}
    \caption[The issue of redundancy in ligand binding site prediction]{\textbf{The issue of redundancy in ligand binding site prediction.} \textbf{(A)} A set of predictions where 6/10 predictions are redundant, resulting in a low recall (1/5) and inflated precision (7/7); \textbf{(B)} When redundancy is removed, only four predictions remain and recall increases to 3/5 (60\%) and precision decreases to 3/4 (75\%).}
    \label{fig:prediction_redundancy}
\end{figure}

\FloatBarrier

\autoref{fig:prediction_redundancy} shows an example protein with $N$ = 5 observed pockets. A method returns 10 predictions, but the top-7 are all within 3\AA{} of one of the observed pockets, and $>$12\AA{} from any of the other four observed pockets. If the top-$N$+2 (top-7) predictions were considered, this would only recall a single unique pocket, as six of the top-7 predictions are redundant. Recall would then be 1/5 (20\%). Precision, however, within this top-7 would be 7/7 (100\%), as the seven predictions are correctly recalling an observed pocket (which happens to be the same observed pocket). In this case, both the low recall and the high precision are artifacts resulting of the redundancy (\autoref{fig:prediction_redundancy}A). Redundancy in prediction can often result in an overestimate of the precision and an underestimate of the recall. \autoref{fig:prediction_redundancy}B illustrates what happens when redundant predictions are removed, keeping always higher-scoring predictions. When the six redundant predictions (blue hexagons) are removed, the other three predictions, which are of different pockets are considered as now fall within the top-$N$+2 predictions. This increases the recall to 60\%, as 3/5 observed pockets are now correctly predicted. However, the precision decreases, as only three out of the four predictions made overlap with an observed pocket. Pocket rank \#2 has a high score but is not observed. This is a \textit{false positive} in this context, however it might be a candidate pocket yet to be resolved and could prove interesting as a drug target.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_COMP/MAIN/PNG/FIG6_PREDICTION_REDUNDANCY_SPLIT_2.png}
    \caption[Example of redundant predictions]{\textbf{Example of redundant predictions.} Predictions by VN-EGNN, IF-SitePred and PUResNet, on chain D of PDB: 4Z9M (Rabeh WM, Tempel W, Nedyalkova L, Landry R, Arrowsmith CH, Edwards AM, Bountra C, Bochkarev A, Park H, Structural Genomics Consortium (SGC), 2015), where ADP binds. For this ADP binding site, VN-EGNN reports 7 predictions, IF-SitePred 33, and PUResNet a single prediction. These three methods correctly predict this site, however, VN-EGNN and IF-SitePred report redundant pocket predictions, which centroids are very close, $\leq$ 5\AA{}, in space and residues overlap high ($\geq$ 0.75).}
    \label{fig:prediction_redundancy_examples}
\end{figure}

\autoref{fig:prediction_redundancy_examples} showcases human creatine kinase S-type, mitochondrial (PDB: 4Z9M) as an example of this phenomenon, where VN-EGNN and IF-SitePred redundantly predict the same pocket 9 and 33 times, whereas PUResNet returns a single prediction. All three methods correctly predict the site, just the difference is in the number of returned predictions.

\section{Methods}

\subsection{Generation of ``non-redundant`` sets of predictions}

Figure 5D shows that prediction redundancy is an issue particularly for VN-EGNN, IF-SitePred, and to a lesser extent DeepPocket\textsubscript{SEG}. To assess the effect that redundancy has on the performance of these methods, non-redundant subsets of predictions were obtained and labelled with the subscript ``NR''. A predicted pocket $i$ is considered redundant if there exists a pocket $j \neq i$ so that the distance between their centroids $D_{i,j} \leq$ 5\AA{} or their residue overlap $JI_{i,j} >$ 0.75, i.e., they share at least 3/4 of their residues. Refer to Supplementary Figure 9 for the closest predicted sites for each method. Redundancy filtering was carried out for each method keeping always the higher scoring pocket. Redundancy (\%) was calculated as the proportion of redundant pockets relative to the original total number of pockets. VN-EGNN presents the highest percentage of redundant pockets with 9,066/13,582 (67\%) redundant pockets, followed by IF-SitePred with 22,232/44,948 (49\%), and DeepPocket\textsubscript{SEG} with 6,744/21,718 (31\%). For other methods, redundancy was minimal ($<$1\%).

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_COMP/SUPP/PNG/SUPP_FIG9_CLOSESET_PREDICTED_POCKETS.png}
    \caption[Closest predicted pockets for each methods]{\textbf{Closest predicted pockets for each method.} For each method, the two closest predicted pockets across all protein chains are shown. This is the pair of pockets with the minimum Euclidean distance between their centroids. Protein surface is coloured in tan. The larger pocket (more residues)}
    \label{fig:closest_pred_pockets}
\end{figure}

\begin{figure}[ht!]
\ContinuedFloat
\caption*{(continued)  and centroid is coloured in the method colour, and the other in grey. A distance threshold of D = 5\AA{} was selected to determine whether a pocket prediction was redundant. LIGYSIS, VN-EGNN, IF-SitePred and DeepPocket clearly differ from other methods presenting distances $<$ 1\AA{}.}
\end{figure}

\subsection{Strategies for the re-scoring of pockets}

PUResNet, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+} do not score, nor explicitly rank their pockets, and so they were taken in the order given by their pocket ID. This means that when sorting across the dataset, the order of all pockets with the same rank is arbitrary. To obtain a score for these pockets, multiple strategies were employed. Firstly, a pocket score was obtained as the number of pocket amino acids, resulting in variants PUResNet\textsubscript{AA}, PocketFinder\textsuperscript{+}\textsubscript{AA}, Ligsite\textsuperscript{+}\textsubscript{AA} and Surfnet\textsuperscript{+}\textsubscript{AA}. Secondly, PRANK pocket scoring was employed, resulting in variants PUResNet\textsubscript{PRANK}, PocketFinder\textsuperscript{+}\textsubscript{PRANK}, Ligsite\textsuperscript{+}\textsubscript{PRANK} and Surfnet\textsuperscript{+}\textsubscript{PRANK}. fpocket predictions were also re-scored with PRANK, resulting in fpocket\textsubscript{PRANK}. Additionally, for IF-SitePred pocket scores were calculated as the sum of squares (SS) of the ligandability scores ($LS_{i}$), calculated with (Equation \ref{eq:IFSP_score}), of the $K$ residues on a site (Equation \ref{eq:IFSP_pocket_score}) resulting in IF-SitePred\textsubscript{RESC}. For PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+}, and Surfnet\textsuperscript{+} the same was done but instead of residue scores, grid point scores ($GS_{i}$) were used (Equation \ref{eq:leagcy_methos_pocket_score}). This resulted in further variants PocketFinder\textsuperscript{+}\textsubscript{SS}, Ligsite\textsuperscript{+}\textsubscript{SS}, and Surfnet\textsuperscript{+}\textsubscript{SS}. This is the same approach introduced by Krivák, \textit{et al.} \cite{KRIVAK_2015_P2RANK} and later adopted by Smith \textit{et al.} \cite{SMITH_2024_GrASP}.

\begin{equation}
SS_{\text{IF--SitePred}} = \sum_{i=1}^{K} LS_i^2
\label{eq:IFSP_pocket_score}
\end{equation}

\begin{equation}
SS_{\text{PocketFinder}^+} = SS_{\text{Ligsite}^+} = SS_{\text{Surfnet}^+} = \sum_{i=1}^{K} GS_i^2
\label{eq:leagcy_methos_pocket_score}
\end{equation}

\subsection{Performance evaluation}

\FloatBarrier

\section{Results}

\subsection{Effect of redundancy and pocket score on ranking}

Because of $K$ = 8 virtual nodes are used in the default VN-EGNN implementation, a maximum of $N$ = 8 predicted pockets are possible. However, only seven are observed in our dataset, as in all cases at least one virtual node gets clustered with another, resulting in 7 ``unique'' predictions. Supplementary Figure 3A illustrates the issue of redundancy in pocket predictions and how it affects the scoring and ranking of the pockets. A prediction of the same pocket is reported multiple times as distinct virtual nodes, or pocket centroids, which are very close to each other, and present very similar scores. This is why there is no apparent difference in the distribution of scores across the ranks for VN-EGNN, unlike all other methods. After removing redundancy and obtaining VN-EGNN\textsubscript{NR}, this is no longer the case (Supplementary Figure 3B).

IF-SitePred predictions are also highly redundant, however, these predictions, despite being close to each other, will present different scores (number of points), that is why higher ranks (1, 2, 3...) present higher scores (Supplementary Figure 3C). The redundancy removal can be identified in Supplementary Figure 3D as the scatter plot is less crowded and the maximum rank across the dataset is 60, as opposed to 120. Supplementary Figure 3E shows the non-redundant set of re-scored IF-SitePred predictions, IF-SitePred\textsubscript{RESC-NR}. This score distribution is wider, i.e., scores take values from a larger distribution of values, which might yield a more relevant scoring of pockets.

There is no clear difference between Supplementary Figure 3G-H, meaning that using PRANK to score PUResNet predictions does not alter the ranking of the predictions made within a protein. This makes sense, as only 10\% of proteins present $>$1 predicted pocket. This new score, however, could help in the ranking of pockets across the dataset, and not just within a protein.

The distribution of scores does not change when removing the redundancy from DeepPocket\textsubscript{SEG} predictions (Supplementary Figure 3I-J), but the maximum rank goes from 200 to 140 indicating the decrease in total predictions. The score distributions of fpocket\textsubscript{PRANK} (Supplementary Figure 3N) and fpocket (Supplementary Figure 3O) are completely different which means the ranking of pockets, and therefore recall and precision might differ considerably between these two scoring schemes of the same predictions.

The score distributions of ``\textsubscript{AA}'', ``\textsubscript{SS}'' and ``\textsubscript{PRANK}'' variants of PUResNet, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+} are similar, suggesting that the number of pocket amino acids might dictate the order in which these pockets are reported Supplementary Figure 3P-X.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_COMP/SUPP/PNG/SUPP_FIG3_SCORE_vs_RANK.png}
    \caption[Pocket score \textit{vs} pocket ranking]{\textbf{Pocket score \textit{vs} pocket ranking.}}
    \label{fig:pocket_score_vs_rank}
\end{figure}

\subsection{Effect of redundancy and pocket score on recall}

Supplementary Note 2 and Figure 3 demonstrate how removing redundancy from predictions can have a drastic effect in the ranking of the predictions, with VN-EGNN being the clearest example. We wanted to explore how redundancy removal affects recall as well as the effect that different pocket scoring schemes might have on recall for those methods that do not report a pocket score: PUResNet, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}.

Supplementary Figure 4A-B shows a significant +5.2\% increase in recall when removing redundancy for VN-EGNN predictions (Recall = 46.1\%), which corresponds to 346 extra predictions that fall within the top-$N$+2 after removing redundancy. An even stronger improvement can be observed for IF-SitePred (Supplementary Figure 4C-D), where a combination of redundancy removal and pocket re-scoring (Eq. 10) results in a significant increase of +13.4\% (Recall = 39.1\%), corresponding to 901 added predictions falling in the considered top-$N$+2 predictions. Most of this change is due to the redundancy removal, as can be seen by the higher recall of IF-SitePred\textsubscript{NR}. Scoring of PUResNet predictions using the number of pocket amino acids (PUResNet\textsubscript{AA}) nor PRANK (PUResNet\textsubscript{PRANK}) had no effect on the recall. This was expected as PUResNet predicts a single pocket in 90\% of the cases, and therefore, there is no strong need for a score to sort predictions within a protein. (Supplementary Figure 4G-H). Just like VN-EGNN and IF-SitePred, the recall of DeepPocket\textsubscript{SEG} benefits from redundancy removal, increasing by +5.7\% (Supplementary Figure 4J-K) with a final recall = 49.4\% (+377 pockets within ranking threshold).

Using PRANK to re-score fpocket predictions was the most impactful across the methods, increasing recall by 13.9\% (recall = 60.4\%), placing it at the top of the list and increasing the number of correct predictions within the rank threshold by +934 (Supplementary Figure 5A-B). For PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}, none of the variants had a significant improvement in the recall (Supplementary Figure 5D-L). This is expected as these predict only a few non-redundant sites per protein, medians ranging 1-3 pockets per protein.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_COMP/SUPP/PNG/SUPP_FIG4_RECALL_VARIANTS_1.png}
    \caption[ROC100 curves for non-redundant and re-scored variants (I)]{\textbf{ROC100 curves for non-redundant and re-scored variants (I).}}
    \label{fig:pocker_recall_variants1}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_COMP/SUPP/PNG/SUPP_FIG5_RECALL_VARIANTS_2.png}
    \caption[ROC100 curves for non-redundant and re-scored variants (II)]{\textbf{ROC100 curves for non-redundant and re-scored variants (II).}}
    \label{fig:pocker_recall_variants2}
\end{figure}

\FloatBarrier

\subsection{Effect of redundancy and pocket score on \# TP\textsubscript{100 FP}}

There are no negative predictions, either True (TN) or False (FN) in the context of ligand binding site prediction at the pocket level and accordingly, standard ROC/AUC curves cannot be obtained. Only positives are predicted (sites). FN can be obtained by examining the observed pockets that are not predicted, but there are not scores for them. ROC100 curves provide an alternative to observe the relationship between True Positives (TP) and False Positives (FP). Predictions for each method across the whole test dataset, LIGYSIS, are sorted based on the pocket scores, and cumulative TP and FPs are counted until a certain number of FP is reached, in this case, 100. This visualisation provides insight into how well high-scoring predictions match the ground truth. A higher number of TP at FP = 100 indicates that the high scoring pockets recapitulate well the ground truth, whereas a low number indicates that the high scoring pockets do not match with the observed data, given the used thresholds of DCC $\leq$ 12\AA{}. It is important to understand that the FP in this context might not always be incorrect predictions, but might be binding sites that are not considered in our ground truth dataset, that is comprised by biologically relevant protein-ligand interactions as defined in BioLiP, or relevant sites that simply have not been experimentally determined yet. It is also important to contextualise this metric with success rate, or recall, i.e., how many of the observed sites are predicted by each method given the above-mentioned threshold, as well as a rank threshold: top-$N$+2. A method might present a high number of TP within the first 100 FP yet have a low recall overall. Supplementary Figure 6 explores how ROC100 changes for the non-redundant ``\textsubscript{NR}'' and re-scored ``\textsubscript{AA}'', ``\textsubscript{PRANK}'', ``\textsubscript{SS}'', and ``\textsubscript{RESC}'' sets of VN-EGNN, IF-SitePred, PUResNet, DeepPocket\textsubscript{SEG}, fpocket, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}.

Supplementary Figure 6A illustrates how redundancy can be misleading and overestimate the performance of VN-EGNN. Removing redundancy results in $\Delta_{TP}$ = \textminus273 (TP = 1,028). This is because redundant predictions by VN-EGNN are very close in space and present very similar scores (Supplementary Figure 3A). Because of this, in the redundant default set of predictions, multiple TP counts are being added for predictions of the same observed pocket. Even with redundancy removed, VN-EGNN reached 1,028 TP for the first 100FP, indicating that the non-redundant higher scoring pockets recapitulate well the observed data.

There is no difference between IF-SitePred and IF-SitePred\textsubscript{NR} (curves overlap completely), which indicates that despite the redundancy in predictions by this method, its scoring scheme can sort sites in a meaningful manner. Let us consider multiple proteins with redundant predictions for IF-SitePred. The scoring scheme allows for the top-1 site of each of these proteins to rank above any of the other redundant predictions of the other proteins. The re-scored and non-redundant set of IF-SitePred predictions, IF-SitePred\textsubscript{RESC-NR}, results in a $\Delta_{TP}$ = +285 (TP = 1,246), indicating that IF-SitePred could benefit from a more sophisticated scoring scheme, rather than the number of cloud points per binding site (Supplementary Figure 6B).

Supplementary Figure 6C shows how important it is to score pocket predictions. PUResNet does not score its predictions. For this reason, within a protein, pockets have been ranked based on the order they are reported. When sorting across the whole dataset, pockets with the same ID or rank where randomly shuffled. A massive increase in TP can be observed when simply sorting by the number of pocket residues, and using PRANK to score this pockets provides an even larger increment in TP ($\Delta_{TP}$ = +563) (TP = 1,097). An application of this could be running PUResNet on a list of potential drug target proteins. It would add great value to be able to rank the predictions among the targets to decide on a target.

The curve does not change much for DeepPocket\textsubscript{SEG}, ($\Delta_{TP}$ = \textminus27) (TP = 643), indicating that despite the segmentation module of DeepPocket might result in overlapping pockets, their scoring scheme is robust. It is important to consider that the pocket score results from re-scoring the fpocket candidates, which are not redundant. The redundancy in DeepPocket\textsubscript{SEG} is therefore unrelated to their scoring scheme. These results suggest that there is a big difference between fpocket candidates and extracted DeepPocket pockets, and it might not be appropriate to consider the score of the former for the latter (Supplementary Figure 6D).

Re-scoring of fpocket predictions with PRANK (fpocket\textsubscript{PRANK}, also has a considerable effect on the ROC100 curve, increasing the number of TP for 100 FP by +432 (TP = 526), proving once more the importance of pocket scoring (Supplementary Figure 6E).

For the last three methods, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}, the results agree in that simply using the number of pocket amino acids results in the maximum TP for 100 FP: $\Delta_{TP}$ = +114 (TP = 178) (Supplementary Figure 6F), $\Delta_{TP}$ = +44 (TP = 159) (Supplementary Figure 6G) and $\Delta_{TP}$ = +247 (TP = 308) (Supplementary Figure 6H). This is surprising, as sum of squares ``\textsubscript{SS}'' and ``\textsubscript{PRANK}'' scoring schemes have worked better for other methods. This result might be related to the fact that pockets predicted by these three methods tend to be larger than those predicted by other methods.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_COMP/SUPP/PNG/SUPP_FIG6_TP100FP_VARIANTS.png}
    \caption[ROC100 curves for non-redundant and re-scored variants]{\textbf{ROC100 curves for non-redundant and re-scored variants.} For each method, predicted pockets across the whole dataset, i.e., all LIGYSIS protein chains, are ranked by their score. This way, pockets with the highest scores will be at the top of the list, whereas pockets with the lowest scores will be at the bottom. This ranking will not correspond to ranking pockets across the dataset by their rank, as a pocket ranked \#2, \#3 or lower could have a higher score than a pocket \#1 of a different protein. Each method has a colour assigned, and each variant resulting of redundancy removal or pocket (re-)scoring a different line style. \textbf{(A)} VN-EGNN and ``\textsubscript{NR}'' variant; \textbf{(B)} IF-SitePred and ``\textsubscript{NR}'', re-scored ``\textsubscript{RESC}'' and ``\textsubscript{RESC-NR}'' variants; \textbf{(C)} PUResNet and ``\textsubscript{AA}'' and ``\textsubscript{PRANK}'' variants; \textbf{(D)} DeepPocket\textsubscript{SEG} and ``\textsubscript{NR}'' variant; \textbf{(E)} fpocket and ``\textsubscript{PRANK}'' variant; \textbf{(F)} PocketFinder\textsuperscript{+} and ``\textsubscript{AA}'', ``\textsubscript{PRANK}'' and ``\textsubscript{SS}'' variants; \textbf{(G)} Ligsite\textsuperscript{+} and variants; \textbf{(H)} Surfnet\textsuperscript{+} and variants.}
    \label{fig:pocket_ROC100_variants}
\end{figure}

\FloatBarrier

\subsection{Effect of redundancy and pocket score on precision}

For the same reason as why ROC/AUC curves cannot be calcualted for ligand binding site prediciton (at the pocket level), precision-recall (PR)/AUC curves cannot be either, as False Negatives (FN) are not predicted, and therefore not scored. Nevertheless, precision, as the ratio of TP/TP+FP, can be measured. For this, as it was done for ROC100, all predictions for a method were sorted by pocket score, and precision calculated as more predictions are considered.

Supplementary Figure 7 portrays the precision curve for the top-1,000 predictions for the non-redundant and re-scored variants for VN-EGNN, IF-SitePred, PUResNet, DeepPocket\textsubscript{SEG}, PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}.

There is no significant ($p >$ 0.05) change in precision between VN-EGNN and VN-EGNN\textsubscript{NR} within the first 1,000 predictions, precision = 91.5\% (Supplementary Figure 7A). The same can be said for IF-SitePred with a precision = 94.3\% (Supplementary Figure 7B). Using PRANK to score PUResNet pockets results in a significant +11.7\% increase in precision of the top-1,000 predictions (precision = 93.3\%) (Supplementary Figure 7C). DeepPocket\textsubscript{SEG-NR}, as the other redundant methods, does not experience a significant change in precision as redundancy is removed (precision = 81.6\%) (Supplementary Figure 7D). PRANK re-scoring of fpocket predictions results in the largest increase in precision across all methods with +34.4\%, reaching a precision of 81.7\% (Supplementary Figure 7E) For PocketFinder\textsuperscript{+}, Ligsite\textsuperscript{+} and Surfnet\textsuperscript{+}, using the number of pocket amino acids results, ``\textsubscript{AA}'', in a precision increase of +23.3\% (precision = 65.3\%), (Supplementary Figure 7F), +16.5\% (precision = 68.8\%) (Supplementary Figure 7G) and 29.1\% (precision = 68.8\%) (Supplementary Figure 7H), respectively. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_COMP/SUPP/PNG/SUPP_FIG7_PRECISION_VARIANTS.png}
    \caption[Precision\textsubscript{1K} curves for non-redundant and re-scored variants]{\textbf{Precision\textsubscript{1K} curves for non-redundant and re-scored variants.}  Precision\textsubscript{1K} represents the precision (\%) calculated for the top-scoring 1,000 predictions. For each method, predicted pockets across the whole LIGYSIS set, are ranked by their score. This way, pockets with the highest scores will be at the top of the list, whereas pockets with the lowest scores will be at the bottom. Each method has a colour assigned, and each scoring variant its own line style. $\Delta_{Precision}$ indicates the difference in precision between the selected method variant and the default one.  \textbf{(A)} VN-EGNN and ``\textsubscript{NR}'' variant; \textbf{(B)} IF-SitePred and ``\textsubscript{NR}'', re-scored ``\textsubscript{RESC}'' and ``\textsubscript{RESC-NR}'' variants; \textbf{(C)} PUResNet and ``\textsubscript{AA}'' and ``\textsubscript{PRANK}'' variants; \textbf{(D)} DeepPocket\textsubscript{SEG} and ``\textsubscript{NR}'' variant; \textbf{(E)} fpocket and ``\textsubscript{PRANK}'' variant; \textbf{(F)} PocketFinder\textsuperscript{+} and ``\textsubscript{AA}'', ``\textsubscript{PRANK}'' and ``\textsubscript{SS}'' variants; \textbf{(G)} Ligsite\textsuperscript{+} and variants; \textbf{(H)} Surfnet\textsuperscript{+} and variants. Error bars indicate 95\% CI of the precision (proportion) and are displayed every 100 predictions.}
    \label{fig:pocket_precision_variants}
\end{figure}

\FloatBarrier

\subsection{Evaluation of predictive performance}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/ch_LBS_COMP/SUPP/PNG/SUPP_FIG8_POCKET_LEVEL_BENCHMARK_IMPROVED.png}
    \caption[Ligand binding site prediction at the pocket level (variants)]{\textbf{Ligand binding site prediction at the pocket level (variants).} Only the top-performing, i.e., highest recall, variant of each method is drawn on this figure, e.g., IF-SitePred\textsubscript{RESC-NR}, VN-EGNN\textsubscript{NR}, or fpocket\textsubscript{PRANK} instead of their default modes. \textbf{(A)} \% Recall, percentage of observed sites that are correctly predicted by a method according to a DCC threshold. Reported recall on \autoref{tab:pocket_level_benchmark_variants} corresponds to DCC = 12\AA{}; \textbf{(B)} \% Recall using DCC = 12\AA{} but considering increasing rank thresholds. \textit{ALL} represents the maximum recall of a method, obtained by considering all predictions, regardless of their rank or score; \textbf{(C)} \% Recall curve using $I_{rel}$ as a criterion; \textbf{(D)} ROC100 curve (cumulative TP against cumulative FP until 100 FP are reached); \textbf{(E)} Precision curve for the top-1,000 predictions of each method across the LIGYSIS dataset. Error bars represent 95\% CI of the recall (A-C) and precision (E), which are 100 $\times$ proportion. Numbers at the right of the panels indicate groups or blocks of methods that perform similarly for each metric. Asterisks ``*'' indicate outlier methods, or methods that perform very differently than the rest.}
    \label{fig:pocket_level_benchmark_variants}
\end{figure}


\begin{landscape}
\begin{longtable}[c]{|M{33mm}|M{27mm}|M{29mm}|M{24mm}|M{27mm}|M{21mm}|M{19mm}|M{19mm}|}
\hline
\textbf{Method}         & \textbf{\% Recall\textsubscript{top-$N$}} & \textbf{\% Recall\textsubscript{top-$N$+2}} & \textbf{\% Recall\textsubscript{max}} & \textbf{\% Precision\textsubscript{1K}} & \textbf{\# TP\textsubscript{100 FP}} & \textbf{\% $RRO$} & \textbf{\% $RVO$} \\ \hline
\endfirsthead
%
\multicolumn{8}{c}%
{{\bfseries Table \thetable\ continued from previous page}} \\
\hline
\textbf{Method}         & \textbf{\% Recall\textsubscript{top-$N$}} & \textbf{\% Recall\textsubscript{top-$N$+2}} & \textbf{\% Recall\textsubscript{max}} & \textbf{\% Precision\textsubscript{1K}} & \textbf{\# TP\textsubscript{100 FP}} & \textbf{\% $RRO$} & \textbf{\% $RVO$} \\ \hline
\endhead
%
VN-EGNN\textsubscript{NR}          & 44.5           & 46.1             & 46.3         & 91.5           & 1,028       & \textbf{\textcolor{firebrick}{27}}     & \textbf{\textcolor{firebrick}{19}}     \\ \hline
IF-SitePred\textsubscript{RESC-NR} & \textbf{\textcolor{firebrick}{29.7}}           & \textbf{\textcolor{firebrick}{39.1}}             & 51           & \textbf{\textcolor{forestgreen}{94.3}}           & \textbf{\textcolor{forestgreen}{1,246}}       & 47     & 37     \\ \hline
GrASP              & 48             & 49.9             & 50           & 92.5           & 1,017       & 56     & 69     \\ \hline
PUResNet\textsubscript{PRANK}      & 40.8           & 41.1             & \textbf{\textcolor{firebrick}{41.1}}         & 93.3           & 1,097       & 65     & 73     \\ \hline
DeepPocket\textsubscript{SEG-NR}   & 43.4           & 49.4             & 55.4         & 81.6           & 643         & 61     & 73     \\ \hline
DeepPocket\textsubscript{RESC}     & 46.6           & 58.1             & 89.3         & 81.7           & 637         & 50     & 41     \\ \hline
P2Rank\textsubscript{CONS}         & 48.4           & 53.9             & 57           & 90.7           & 932         & 57     & 50     \\ \hline
P2Rank             & 46.7           & 51.9             & 57           & 79.2           & 586         & 56     & 66   \\ \hline
fpocket\textsubscript{PRANK}       & \textbf{\textcolor{forestgreen}{48.8}}           & \textbf{\textcolor{forestgreen}{60.4}} & \textbf{\textcolor{forestgreen}{91.3}}         & 81.7           & 526         & 59     & 41     \\ \hline
PocketFinder\textsuperscript{+}\textsubscript{AA}    & 44.5           & 48.9             & 50.5         & \textbf{\textcolor{firebrick}{65.3}}           & 178         & 79     & 97     \\ \hline
Ligsite\textsuperscript{+}\textsubscript{AA}         & 44.9           & 49               & 49.7         & 68.8           & \textbf{\textcolor{firebrick}{159}}         & \textbf{\textcolor{forestgreen}{88}}     & \textbf{\textcolor{forestgreen}{98}}     \\ \hline
Surfnet\textsuperscript{+}\textsubscript{AA}         & 43.3           & 47.3             & 48.9         & 68.6           & 308         & 76     & 92     \\ \hline
\caption[Pocket level evaluation (\textit{best} variants)]{\textbf{Pocket level evaluation (\textit{best} variants).} Only the top-performing, i.e., highest recall, variant of each method is drawn on this figure, e.g., IF-SitePred\textsubscript{RESC-NR}, VN-EGNN\textsubscript{NR}, or fpocket\textsubscript{PRANK} instead of their default modes. \% Recall for each method considering top-$N$, $N$+2 and all predictions (max) without taking rank into consideration, i.e., maximum recall. \% Precision of the method for the top-1,000 scored predictions. Number of TP reached for the first 100 FP (\# TP\textsubscript{100 FP}). Median \% relative residue overlap ($RRO$) for those sites correctly predicted and \% relative volume overlap ($RVO$) only for correctly predicted sites that have a volume, i.e., are pockets or cavities, and not exposed sites, which do not have a volume. These last two metrics represent the overlap in residues and volume relative to the observed site. Bold font indicates the best (green) and worst (red) performing methods for each metric.}
\label{tab:pocket_level_benchmark_variants}\\
\end{longtable}
\end{landscape}

\section{Discussion}

We have shown how redundancy in prediction, i.e., predicting multiple times the same observed site, can underestimate the recall, and overestimate the precision of the methods, therefore providing a misleading assessment of the methods’ performance. Redundancy removal and subsequent pocket re-ranking can yield a significant increase in recall. The importance of a robust pocket scoring scheme can have a strong impact in the performance, both in recall and precision of the methods and emphasis should be put into this area. Even if a single site is predicted per protein, a pocket score can be highly useful when ranking pockets in different proteins, e.g., when having a list of potential drug targets and deciding which protein might be best to target therapeutically.

fpocket\textsubscript{PRANK} (60\%) and DeepPocket\textsubscript{RESC} (58\%) present the highest recall of the methods reviewed in this work. P2Rank\textsubscript{CONS} and P2Rank follow closely with 54\% and 52\% recall, then GrASP (50\%), DeepPocket\textsubscript{SEG-NR}, Ligsite\textsuperscript{+}\textsubscript{AA} and PocketFinder\textsuperscript{+}\textsubscript{AA} with 49\%, Surfnet\textsuperscript{+}\textsubscript{AA} (47\%), VN-EGNNNR (46\%), PUResNet\textsubscript{PRANK} (41\%) and IF-SitePred\textsubscript{RESC-NR} (39\%). fpocket is the method that predicts the most pockets per protein, reaching a maximum recall between 80-90\% (considering all pockets regardless of the rank). P2Rank\textsubscript{CONS} comes second with a maximum recall of 50-60\%. The rest of the methods range 40-55\%. This indicates that whilst there are still some pockets un-predicted by fpocket (10-20\%), the maximum recall of this method is 20-30\% higher than any other method. However, considering top-$N$+2 pockets, fpocket only recalls 47\% of the observed pockets. fpocket\textsubscript{PRANK} and DeepPocket\textsubscript{RESC} gain $>$10\% in recall by simply re-scoring those predictions. This highlights the paramount importance of a robust scoring scheme, which captures well the nature of binding sites and places those with a higher probability of being real binding sites at the top of the ranking. Newer methods like VN-EGNN, IF-SitePred, GrASP and PUResNet are the most precise methods, however because of redundancy in predictions (VN-EGNN, IF-SitePred), or low number of predicted pockets per protein (VN-EGNN, GrASP and PUResNet) are limited in their recall. Their high precision indicates that their models learn and capture well the nature of ligand binding sites and so they represent a great venue to pursue in the field of ligand binding site prediction. Whilst removing redundancy post-prediction has a significant improvement in performance (VN-EGNN\textsubscript{NR} and IF-SitePred\textsubscript{NR}), approaching this issue before prediction would be more beneficial. For VN-EGNN, which predicts a maximum of 8 sites, ensuring these 8 (or more) predictions are non-redundant is more desirable than removing redundant predictions ending up with 1/8 predictions. The same applies to IF-SitePred, where non-overlapping starting predictions are more convenient than dealing with redundancy post-prediction.

\section{Conclusions}

The conclusions resulting from our analysis are as follows:

\begin{itemize}

\item Redundancy in ligand binding site prediction leads to an underestimate of recall and an overestimate of precision. The removal of such redundancy and subsequent re-ranking of the remaining pockets results in a drastic increase in recall.

\item A robust pocket scoring scheme is crucial for the correct ranking and prioritisation of predicted sites in downstream analysis, e.g., docking, simulation. Additionally, it has a significant positive effect on both precision and recall.

\item Re-scoring of fpocket predictions, as fpocket\textsubscript{PRANK} or DeepPocket\textsubscript{RESC} present the highest recall (60\%) among the methods reviewed in this analysis.

\item IF-SitePred benefits significantly from pocket re-scoring, and suggests that protein embeddings, which are not directly dependent of structure, represent great promise in the field of ligand site prediction.

\end{itemize}

